Scoring Foundations
=====

```{r ,results="hide", echo=FALSE}
library(knitr)
opts_knit$set(base.dir = "scoring")
```

### The intuition behind

> Events can occur, or not... altough we don't have _tomorrow's newspaper_ :newspaper:, we can make a good guess about how is it going to be.

The future is undoublty attached to *uncertanty*, and this uncertanty can be estimated.

<br>

### And there are diferents targets...

By now this book will cover the classical: `Yes`/`No` target -also known as binary or multiclass prediction.

So, this estimation is the _value of truth_ of an event to happen, therefore a probabilist value between 0 and 1.

<br>

### Hey what? 

Some examples:
- Is this client going to buy this product?
- Is this patient going to get better?
- Is certain event going to happen in the next weeks? 

The answers to last questions are True or False, but **the essense is to have an score**, or a number indicating the likelihood of certain event to happen.

<br>

### But we need more control...

Many machine learning resources shows the simplified version -wich is good to start- getting the final class as an output. Let's say:

Simplified approach:

* Question: _Is this person going to have a heart disease?_ 
* Answer: "No"

But there is something else before the "Yes/No" answer, and this is the score:

* Question: _What is the likelihood for this person of having heart disease?_
* Answer: "25%"

<br>
So first you get the score, and then according to your needs you set the **cut point**. And this is **really** important.


### Let see an example
<img src='tbl_example_1.png' width='400px'> 

Example table showing the following
* `id`=identity
* `x1`,`x2` and `x3` input variables
* `target`=variable to predict


<img src='tbl_example_2.png' width='250px'> 

Forgetting about input variables... After the creation of the predictive model, like a random forest, we are interested in the **scores**. Even though our final goal is to deliver a `yes`/`no` predicted variable.


For example, following 2 sentences express the same: _The likelihood of being `yes` is `0.8`_ <=> _The likelihood of being `no` is `0.2`_

May be it is understood, but the score usually refers to the less representative class: `yes`.

--- 

**R Syntax** -_skip it if you don't want to see code_-

Following sentence will return the score:

`score = predict(randomForestModel, data, type = "prob")[, 2]`

Please note for other models this sintax may vary a little, but the concept **will remain the same**. Even for other languages.

Where `prob` indicates we want the probabilities (or scores). 

The `predict` function + `prob` parameter returns a matrix of 15 rows and 2 columns: the 1st indicates the likelihood of being `no` while the 2nd one indicates the same for class `yes`.

Since target variable can be `no` or `yes`, the `[, 2]` return the likelihood of being -in this case- `yes` (which is the complement of the `no` likelihood).

--- 

<br>

### It's all about the cut point

<img src='tbl_example_3.png' width='250px'> 

Now the table is ordered by score descendently.

It's to see how to extract the final class having by default the cut point in `0.5`. Tweeking the cut point will lead into a better classification.

> Accuracy metric or confusion matrix are always attached to a certain cut point value.

<br>

After assigning the cut point, we can see the clasification result getting the famous: 

* :white_check_mark:**True Positive** (TP): It's _true_, that the classification is _positive_, or, "the model hitted correctly the positive (`yes`) class".
* :white_check_mark:**True Negative** (TN): Same as before, but with negative class (`no`).
* :x:**False Positive** (FP): It's _false_, that the classification is _positive_, or, "the model missed, it predicted `yes` but the result was `no`
* :x:**False Negative** (FN): Same as before, but with negative class, "the model predicted negative, but it was positive", or, "the model predicted `no`, but the class was `yes`"


<img src='tbl_example_4.png' width='500px'> 

<br>

### The best and the worst escenario

> The analysis of extremes will help to find the right balance

:thumbsup: The best escenario is when **TP** and **TN** rates are 100%. That means the model correctly predicts all the `yes` and all the `no`; _(as a result, **FP** and **FN** rates are 0%)_.

But wait :raised_hand:! If you find a perfect classification, probably it's because of overfitting!

:thumbsdown: The worst escenario -the opposite to last example- is when **FP** and **FN** rates are 100%. Not even randomness can achieve such an awful escenario. 

_Why?_ If classes are balanced, 50/50, flipping a coin will assert around half of the results. This is common baseline to test if the model if better than randomness.

<br>
<br>

In the example provided, class distribution is 5 for `yes`, and 10 for `no`; so: 33,3% (5/15) is `yes`. 

<br>

---

### Comparing classifiers

#### Comparing classification results

:question: **Trivia**: Does a model which correcltly predict this 33.3% (TP rate=100%) a good one?

_Answer_: It depends on your needs. 


A classifier that always predicts `yes`, will have a TP of 100%, but is absolutly useless since the FP rate will be quite high -a lot of `yes` will be actually `no`.

#### Comparing ordering label based on score 

A classifier must be trustful, and this is what **ROC** curves measures when plotting the TP rates against the FP. The focus is on how well the score orders the label, ideally all the positive labels must be at the top, and the negative ones at the bottom. 


<img src='tbl_example_5.png' width='500px'> 

<br>

`model 1` will have a higher Area Under Roc Curve (AUC) than `model 2`.

üëç











