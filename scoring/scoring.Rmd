Scoring Foundations
=====

```{r ,results="hide", echo=FALSE}
library(knitr)
opts_knit$set(base.dir = "scoring")
```

### The intuition behind

Events can occur, or not... altough we don't have _tomorrow's newspaper_, we can make a good guess about how is it going to be.

The future is undoublty attached to *uncertanty*, and this uncertanty can be estimated.

### And there are diferents targets...

By now this book will cover the classical: Yes/No target -also known as binary or multiclass prediction.

So, this estimation is the _value of truth_ of an event to happen, therefore a probabilist value between 0 and 1.

### Hey what? 

Some examples:
- Is this client going to buy this product?
- Is this patient going to get better?
- Is certain event going to happen in the next weeks? 

The answers to last questions are True or False, but **the essense is to have an score**, or a number indicating the likelihood of certain event to happen.

### But we need more control...

Many machine learning resources shows the simplified version -wich is good to start- getting the final class as an output. Let's say:

Simplified approach:

* Question: _Is this person going to have a heart disease?_ 
* Answer: "No"

But there is something else before the "Yes/No" answer, and this is the score:

* Question: _What is the likelihood for this person of having heart disease?_
* Answer: "25%"

<br>
So first you get the score, and then according to your needs you set the **cut point**. And this is **really** important.


### Let see an example
<img src='tbl_example_1.png' width='400px'> 

Example table showing the following
* `id`=identity
* `x1`,`x2` and `x3` input variables
* `target`=variable to predict


<img src='tbl_example_2.png' width='250px'> 

Forgetting about input variables... After the creation of the predictive model, like a random forest, we are interested in the **scores**. Even though our final goal is to deliver a `yes`/`no` predicted variable.

Following sentence will return the score:

`score = predict(randomForestModel, data, type = "prob")[, 2]`

Please note for other models this sintax may vary a little, but the concept **will remain the same**. Even for other languages.

Where `prob` indicates we want the probabilities (or scores). 

The `predict` function + `prob` parameter returns a matrix of 15 rows and 2 columns: the 1st indicates the likelihood of being `no` while the 2nd one indicates the same for class `yes`.

Since target variable can be `no` or `yes`, the `[, 2]` return the likelihood of being -in this case- `yes` (which is the complement of the `no` likelihood).

For example, following 2 sentences express the same: _The likelihood of being `yes` is `0.8`_ <=> _The likelihood of being `no` is `0.2`_

May be it is understood, but the score usually refers to the less representative class: `yes`.

### It's all about the cut point

<img src='tbl_example_3.png' width='250px'> 

Now the table is ordered by score descendently.

Several examples only mention how to extract the final class, may be due to the default cut point in 0.5 that almost every alghoritm have, leading this to a not-optimal result. 

**Accuracy Metric or Confution Matrix must always be attached to a certain cut point.**

<br>
