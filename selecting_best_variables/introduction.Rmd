Selecting Variables Introduction
===

### What is this about?

This chapter will cover three types of plots which aim to understand what are the most correlated numeric variables against a target variable.


```{r ,results="hide", echo=FALSE}
library(knitr)
#opts_knit$set(base.dir = "selecting_best_variables")
```

* **Overview**:
    + **Analysis purpose**: To identify if the input variable is a good/bad predictor through visual analysis. 
    + **General purpose**: To explain the decision of including -or not- a variable to a model to a *non-analyst* person. 

<br>


### The "best" selection?

The chapter says "best", but we'd better mention a conceptual point: 

In general terms: _There is no unique best variable selection._ 

To start from this perspective is important, since in the exploration of many algorihms that _rank_ the variables according to their predictive power we can found different -and similar- results. That is

* Algorithm 1 has choosen as the best variable `var_1`, following by `var_5` and `var_14`.
* Algorithm 2 did this ranking: `var_1`, `var_5` and `var_3`.

Let's imagine based on algorithm 1, accuracy is 80%, while the accuracy based on algorithm 2 is 78%. Considering that every model has its inner variance, the result can be considered as the same. It reduces us time in pursuit of the perfect variable selection.

However going to the extremes, there will be a set of variables that will rank high across many algorithms, and the same goes for those with low predictive power. After several runs most reliable variables will emerge quickly, so: 

**Conclusion**: if results are not good the focus should be on improving and/or checking the **data preparation** step. _Next section will exemplify it._ 

<br>

### Going deeper into variable ranking

It's quite common to find in literature and algorithms of selecting best variables an univariate analysis report of them, that is a ranking of variables given certain metric.

We're going to create two models: random forest and gradient boosting machine using `caret` R package to cross-validate the data. Next we'll compare the best variable ranking that every model return.

```{r, eval=FALSE}
library(caret)
library(funModeling)
library(dplyr)

## Excluding all NA rows from data, in this case NA are not the main issue to solve, we'll skip the 6 cases which have NA.
heart_disease=na.omit(heart_disease)

## setting cross-validation 4-fold
fitControl = trainControl(method = "cv",
                           number = 4,
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary)

## creating the random forest model finding the best tunning parameter set
set.seed(999)
fit_rf = train(x=select(heart_disease, -has_heart_disease, -heart_disease_severity),
             y = heart_disease$has_heart_disease,
             method = "rf",
             trControl = fitControl,
             verbose = FALSE,
             metric = "ROC")

## creating the random forest model finding the best tunning parameter set
fit_gbm = train(x=select(heart_disease, -has_heart_disease, -heart_disease_severity),
             y = heart_disease$has_heart_disease,
             method = "gbm",
             trControl = fitControl,
             verbose = FALSE,
             metric = "ROC")
```

Now we can proceed with the comparison. Columns: `importance_rf` and `importance_gbm` represent the importance measured by each algorithm. Based on each metric, there is the `rank_rf` and `rank_gbm` which represent the importance order, finally `rank_diff` represents how differnt each rank according comparing two algorithms.


```{r, eval=FALSE}
## next code is not important, it's creates the table described before...
var_imp_rf=data.frame(varImp(fit_rf, scale=T)["importance"]) %>% dplyr::mutate(variable=rownames(.)) %>% dplyr::rename(importance_rf=Overall) %>% dplyr::arrange(-importance_rf) %>% dplyr::mutate(rank_rf=seq(1:nrow(.))) 

var_imp_gbm=as.data.frame(varImp(fit_gbm, scale=T)["importance"])  %>% dplyr::mutate(variable=rownames(.)) %>% dplyr::rename(importance_gbm=Overall) %>% dplyr::arrange(-importance_gbm) %>% dplyr::mutate(rank_gbm=seq(1:nrow(.)))                                                                                                                             
final_res=merge(var_imp_rf, var_imp_gbm, by="variable")

final_res$rank_diff=final_res$rank_rf-final_res$rank_gbm

# Printing the results!
final_res
```
<img src="ranking_best_vars_comparison.png" alt="Comparisson across two methods for variable ranking">

We can see that there are variables which are not important at all to both models (`fasting_blood_sugar`). There are others that mantain a position at the top of importance like `chest_pain` and `thal`.

Different predictive models implementations have their own criteria to report what are the best features, according to that particular model. This ends up in different ranking across different algorithms. _More info about the inner importance metrics  can be found at <a href=https://topepo.github.io/caret/variable-importance.html" target="blank">caret documentation</a>._

Even more, in tree based model like GBM and Random Forest there is a random component to pick up variables, and the importance is based on prior -and automatic- variable selection when building the trees. The importance of each variable depends on the others, not only on its isolated contribution: **Variables work in groups**. We'll back on this later on this chapter.

Altough the ranking will vary from algorithm to algorithm, in general terms there is a correlation between all of these results as we mentioned before. 

**Conclusion:** Every ranking list is not the _"final truth"_, it gives us orientation about where the information is. 

<br>


### The nature of the selection

There are two main approaches when getting the variable selection:

**Predictive model dependent**: 

Like the ones we saw before, this is the most common. The model will rank variables according to one instrinsic measure of accuracy. In tree based models metrics such as information gain, gini index, node impurity. Ref [4], [5].

**Not predictive model dependent**: 

This is quite interesting since they are no as popular as the other ones, but they are prooved to perform really well in areas realated to genomic data. They need to find those _relevant_ genes (input variable) that are correlated with certain disease, like cancer (target variable).

This area differs from others most common in the amount of variables to analyze (in the order of thousands), really huge in comparisson with others. 
 
One algorithm to perform this is <a href="http://home.penglab.com/proj/mRMR/" target="blank">mRMR</a>, acronym for Minimum Redundancy Maximum Relevance Feature Selection. It has its own implementation in R in <a href="https://cran.r-project.org/web/packages/mRMRe/vignettes/mRMRe.pdf" target="blank">mRMRe</a> package.

<br> 

### Improving variables

Variables can increase their predictive power by treating them. 

This book covers by now:

* <a href="http://livebook.datascienceheroes.com/data_preparation/high_cardinality_predictive_modeling.html" target="blank">improvement of categorical variables</a>.
* Reducing the noise in numerical variables through binning in <a href="http://livebook.datascienceheroes.com/selecting_best_variables/cross_plot.html"  target="blank">cross_plot</a> function.
* <a href="http://livebook.datascienceheroes.com/data_preparation/outliers_treatment.html" target="blank">Preparing outliers</a> for predictive modeling.

_And more to come..._

<br>

### Cleaning by domain knowledge

This point is excluded from algorithmic procedures, it is related to the area in which the data was generated.

Imagine data coming from a survey. This survey has 1 year of history and during the first 3 months there wasn't a good control when inserting data, so users can type whatever they want. Variables during this period will probably be spourious. 

Easy to recognize when during an specific period data is empty or null. Also when there are extreme values. 

We should ask question: 

_Is it reliable this data?_ Keep in mind the predictive model will learn _as a kid_, it will not judge the data just learn from it. If data is spourious in an specific period, then remove these input cases.

To move on this point, we should get in touch a little with every input variable.

<br>

### Variables work in groups

<img src="variable_groups.png" width="300px" alt="Variable work in groups">

When selecting the _best_ variables, the main aim is to get those variables which carry the most information regarding a target, outcome or dependant variable. 

A predictive model will find its weights or parameters based on its 1 to 'N' input variables.

Variables usually don't work isolatelty when explaining an event. Quoting Aristotle: 

> “The whole is greater than the sum of its parts.” 

This is also true when selecting the _best_ features: 

_Building a predictive model with two variables may reach a higher accuracy than the models built with only one variable._

For example: Building a model based on variable `var_1` could lead to an overall accuracy of 60%. On the other hand build a model based on `var_2` could reach an accuracy of 72%. But when we combine these two `var_1` and `var_2` variables, we could reach an accuracy above 80%.

<br>

#### Example in R: Variables working in groups

<img src="aristotle.png" width="300px" alt="Aristotle: philosopher and data scientist">

Following code ilustrate what Aristotle said _some_ years ago. 

It creates 3 models based on different subset of variable:

* model 1 is based on `max_heart_rate` input variable
* model 2 is based on `chest_pain` input variable
* model 3 is based on `max_heart_rate` **and** `chest_pain` input variables

Each model returns the metric ROC, and the result contains the improvement of considering the two variables at the same time vs taking each variable isoletly.

```{r, eval=TRUE, message=F}
library(caret)
library(funModeling)
library(dplyr)

## setting cross-validation 4-fold
fitControl = trainControl(method = "cv",
                          number = 4,
                          classProbs = TRUE,
                          summaryFunction = twoClassSummary)

create_model<-function(input_variables) {
  ## create gradient boosting machine model based on input variables
  fit_model = train(x=select(heart_disease, one_of(input_variables)),
              y = heart_disease$has_heart_disease,
              method = "gbm",
              trControl = fitControl,
              verbose = FALSE,
              metric = "ROC")
  
  # returning the ROC as the performance metric
  max_roc_value=max(fit_model$results$ROC)
  return(max_roc_value)
}

roc_1=create_model("max_heart_rate")
roc_2=create_model("chest_pain")
roc_3=create_model(c("max_heart_rate", "chest_pain"))

avg_improvement=round(100*(((roc_3-roc_1)/roc_1)+((roc_3-roc_2)/roc_2))/2,2)
avg_improvement_text=sprintf("Average improvement: %s%%", avg_improvement)

results=sprintf("ROC model based on 'max_heart_rate': %s.; based on 'chest_pain': %s; and based on both: %s", round(roc_1,2), round(roc_2,2), round(roc_3, 2))

# printing the results!
cat(c(results, avg_improvement_text), sep="\n\n")
```

<br>

#### Conclusions 💥 

* **Around 9% better**, not bad. This percentage is the result of the variables working in groups.
* This effect appears if the variables contain information, such is the case of `max_heart_rate` and `max_heart_rate`. 
* Putting **noisy variables** next to good variables **will affect** overall performance.
* Also the **work in groups** effect is higher if input variables **are not correlated between** them. This is sort of difficult of optimize in practice. More on this on next section...


<br>

### Correlation between input variables

The ideal escenario is to build a predictive model with only non-correlated variables between them. In practice it's complicated to keep such a escenario for all variables. 

For sure there will be variables a set of variables that are not correlated between them, but also there will be others that share a little of correlation.

**In practice** a suitable solution would be to exclude those variables with a **really high level** of correlation.

Regarding how to measure correlation. Results can be highly different based on linear or non-linear procedures. More info at the <a href="http://livebook.datascienceheroes.com/selecting_best_variables/correlation.html">correlation chapter</a>.

<br>

_What is the problem with adding correlated variables?_

The problem is we're adding complexity to the model: more time consuming, more difficult to understand-explain, less accurate, etc. This is an effect we reviewed in <a href="http://livebook.datascienceheroes.com/data_preparation/high_cardinality_predictive_modeling.html#dont-predictive-models-handle-high-cardinality-part-2">reducing cardinality in categorical variables</a>. 


The general rule would be: Try to add top N variables that are correlated with the output, but not correlated between them. This lead us to the next section. 


<br>

### Keep it simple

<img src="fractals_nature.png" alt="Nature operates in the shortest way possible. -Aristotle." width="250px">

> Nature operates in the shortest way possible. -Aristotle.

The principle of **Occam's razor**: Among competing hypotheses, the one with the fewest assumptions should be selected.

Re-interpreting this sentence for machine learning, those "hypotheses" can be seen as variables, so we've got: 

**Among different predictive models, the one with fewest variables should be selected.**

Of course, there is also the trade-off of adding-substracting variables and the accuracy of the model. 

A predictive model with a _high_ number of variables will tend to do **overfitting**. while on the other hand, a model with a _low_ number of variables will lead to do **underfitting**.

The concept of _high_ and _low_ is **highly subjective** to the data that is under analysis. In practice, we may have some accuracy metric, for example ROC value. In practice we would see something like:

<img src="variable_selection_table.png" alt="Quantity of variables vs ROC value trade-off"> 

Last picture shows different subset of variables and an accuracy metric (ROC). Each dot represents the ROC value given certain number of variables used to build the model.

We can check that the highest ROC is reached when the model is built with 30 variables. If we based the selection only in an automated process we may be choosing a subset which tend to overfit data. This report was produced by library `caret` in R [2], but is analogous to any software.

Take a closer look at the difference between the subset of 20 and the 30, there is only an improvement of **1.8%** -from 0.9324 to 0.95- choosing **10 more variables.** In other words: _Choosing 50% more variables will impact in less than 2% of improvement._

Even more, this 2% may be an error margin given the variance in prediction that every predictive model has [3].

**Conclusion:**

In this case, and being consequent with Occams Razor principle, the best solution is to build the model with the subset of 20 variables.

Explaining to others -and understanding- a model with 20 variables is easier than similar one with 30.

<br> 

### Selecting variables in practice

#### The short answer

pick up top _N_ variables from the algorithm you're using and then re-build the model with this subset. Not every predictive model retrieves rankings, but if so, use the same model to get the ranking and to build the final model, try don't mix them. 

When getting the ranking list, try to use predictive models which contains re-sampling, bootstrapping, bagging and perform a cross-validation with repetition. Last terms are based on ensemble methods (like random forest ensembles trees), and b

Ranking based on decision tree doesn't use any 

For those models which like k-Nearest Neighbords which don't have a built-in select best features, we'd use the one returned by other algorithm.

#### The long answer

**The "not so" quick answer:** Try other methods to find groups of variables like the one mentioned before: mRMR.


When possible, **validate** the list with someone who knows about the field study, the business and/or the area from the data comes from. Either to validate the top _N_ and the bottom _M_ variables. Regarding those _bad_ variables we may be missing something in the data munging that could be destroying its predictive power whereas the domain expert advices about the relevance of such variable.

Exclude the lowest ranked variables and focus on the data preparation of the most important. But remember to validate the process and data prepartion of the bad ones before.

<br>

### Summary



<br> 

**References:**

* [1] <a href="https://en.wikipedia.org/wiki/Occam's_razor#Probability_theory_and_statistics">Occam's razor in statistics</a>.
* [2] <a href="https://topepo.github.io/caret/recursive-feature-elimination.html">Recursive feature elimination in caret</a>
* [3] It is covered in the <a href="http://livebook.datascienceheroes.com/model_performance/knowing_the_error.html">Knowing the error</a> chapter.
* [4] Understanding <a href="http://stackoverflow.com/questions/1859554/what-is-entropy-and-information-gain" target="blank">Entropy and Information Gain</a>.
* [5] Understanding the <a href="http://stats.stackexchange.com/questions/197827/how-to-interpret-mean-decrease-in-accuracy-and-mean-decrease-gini-in-random-fore" target="blank">accuracy and gini index</a> used in random forest variable ranking.


