Introduction
===

### What is this about?

This chapter will cover three types of plots which aim to understand what are the most correlated numeric variables against a target variable.


```{r ,results="hide", echo=FALSE}
library(knitr)
#opts_knit$set(base.dir = "selecting_best_variables")
```

* **Overview**:
    + **Analysis purpose**: To identify if the input variable is a good/bad predictor through visual analysis. 
    + **General purpose**: To explain the decision of including -or not- a variable to a model to a *non-analyst* person. 

**Constraint:** Target variable must contain only 2 values. If it has `NA` values, they will be removed.



<br>


### The "best" selection?

The chapter says "best", but we'd better mention a conceptual point: 

In general terms: _There is no unique best variable selection._ 

To start from this perspective is important, since in the exploration of many algorihms that _rank_ the variables according to their predictive power we can found different -and similar- results. That is

* Procedure 1 has choosen as the best variable `var_1`, following by `var_5` and `var_14`.
* Procedure 2 did this ranking: `var_1`, `var_5` and `var_3`.

Let's imagine based on procedure 1, accuracy is 80%, while the accuracy based on procedure 2 is 78%. Considering that every model has its inner variance, the result can be considered as the same.

However going to the extremes, there will be a set of variables that will rank high across many procedures, and the same goes for those with low predictive power. 

<br>

### Ranking variables? There is not only one truth

It's quite common to find in literature and algorithms of selecting best variables an univariate analysis report of them, that is a ranking of variables given certain metric.

TODO: write example of heart_disease data variable ranking based on random forest, the two metrics, and gradient boosting machine.

Different predictive models implementations have their own criteria to report what are the best features, according to that particular model. This ends up in different ranking across different algorithms (such as random forest, gradient boosting machine, etc).

Altough the ranking will vary from algorithm to algorithm, in general terms there is a correlation between all of these results.

In practice as a quick solution: We can pick up top N variables from one of the ranking list, and then build a model with only those.

<br> 

### Improving variables

Variables can increase their predictive power by treating them. This book covers by now the <a href="" target="blank">improvement of categorical variables</a>.



<br>

### Variables work in groups

When selecting the _best_ variables, the main aim is to get those variables which carry the most information regarding a target, outcome or dependant variable. 

A predictive model will find its weights or parameters based on its 1 to 'N' input variables.

Variables usually don't work isolatelty when explaining an event. Quoting Aristoteles: 

> “The whole is greater than the sum of its parts.” 

This is also true when selecting the _best_ features: 

_Building a predictive model with two variables may reach a higher accuracy than the models built with only one variable._

For example: Building a model based on variable `var_1` could lead to an overall accuracy of 60%. On the other hand build a model based on `var_2` could reach an accuracy of 72%. But when we combine these two `var_1` and `var_2` variables, we could reach an accuracy above 80%. TODO: propose example.

<br>



### Correlation between variables


<br>

### Keep it simple

The principle of **Occam's razor**: 

> Among competing hypotheses, the one with the fewest assumptions should be selected.

Re-interpreting this sentence for machine learning, those "hypotheses" can be seen as variables, so we've got: 

**Among different predictive models, the one with fewest variables should be selected.**

Of course, there is also the trade-off of adding-substracting variables and the accuracy of the model. 

A predictive model with a _high_ number of variables will tend to do **overfitting**. while on the other hand, a model with a _low_ number of variables will lead to do **underfitting**.

The concept of _high_ and _low_ is **highly subjective** to the data that is under analysis. In practice, we may have some accuracy metric, for example ROC value. In practice we would see something like:

<img src="variable_selection_table.png" alt="Quantity of variables vs ROC value trade-off"> 

Where we have different subset of variables and an accuracy metric (ROC). Each dot represents the ROC value given certain number of variables used to build the model.

We can check that the highest ROC is reached when the model is built with 30 variables. If we based the selection only in an automated process we may be choosing a subset which tend to overfit data. This report was produced by library `caret` in R [2], but is analogous to any software.

Take a closer look at the difference between the subset of 20 and the 30, there is only an improvement of **1.8%** -from 0.9324 to 0.95- choosing **10 more variables.** In other words: _Choosing 50% more variables will impact in less than 2% of improvement._

Even more, this 2% may be an error margin given the variance in prediction that every predictive model has [3].

**Conclusion:**

In this case, and being consequent with Occams Razor principle, the best solution is to build the model with the subset of 20 variables.

Explaining to others -and understanding- a model with 20 variables is easier than similar one with 30.


<br> 

**References:**

* [1] <a href="https://en.wikipedia.org/wiki/Occam's_razor#Probability_theory_and_statistics">Occam's razor in statistics</a>
* [2] <a href="https://topepo.github.io/caret/recursive-feature-elimination.html">Recursive feature elimination in caret</a>
* [3] It is covered in the <a href="http://livebook.datascienceheroes.com/model_performance/knowing_the_error.html">Knowing the error</a> chapter