Correlation and Information Theory
=====

### Overview


### Linear correlation (classical)

It retrieves the well known `R statistic` (or Pearson coefficient), which measures **linear** correlation  for all numeric variables _(skipping the string ones)_.


```{r ,results="hide", echo=FALSE}
library(knitr)
#opts_knit$set(base.dir = "selecting_best_variables")
```


```{r lib, results="hide"}
## Loading funModeling !
suppressMessages(library(funModeling))
data(heart_disease)
```

```{r}
correlation_table(data=heart_disease, str_target="has_heart_disease")
```

`R statistic` goes from `1` _positive correlation_ to `-1` _negative correlation_. A value around `0` implies no correlation.
Squaring this number returns the `R squared` statistic (aka `R2`), which goes from `0` _no correlation_ to `1` _high correlation_. 

#### R2 bias problem

**R statistic is highly influenced by outliers and non-linear relationships.**

Outliers can be treated with `prep_outliers` function, present in this package.

Take a look at the **Anscombe's quartet**. These 4 relationships are quite different, but all of them have the same R2: 0.816.


```{r, out.width = 400, fig.retina = NULL, echo=F}
library(knitr)
include_graphics("anscombe_quartet.png")
```



Last plot, and more info about correlation can be found at: [Correlation and dependence](https://en.wikipedia.org/wiki/Correlation_and_dependence)

<br>

### Correlation based on Information Theory

This relationships can be measure better with <a href="https://en.wikipedia.org/wiki/Information_theory">Information Theory</a> conepts. One of the many algortihms to measure correlation based on this is: **MINE**, acronym for: Maximal Information-based nonparametric exploration.

The implementation in R can be found in <a href="https://cran.r-project.org/web/packages/minerva/index.html">minerva</a> package.


<br>

#### An example in R: Finding reliable variables

```{r}
library(funModeling)
library(minerva)
library(ggplot2)
options(scipen=999)

# Important: If you recieve this message `Error: Missing values present in input variable 'x'. Consider using use = 'pairwise.complete.obs'.` is because data has missing values.

x=seq(0, 20, length.out=500)
df_exp=data.frame(x=x, y=dexp(x, rate=0.65))
ggplot(df_exp, aes(x=x, y=y)) + geom_line()

# position [1,2] contains the correlation of both variables, excluding the correlation measure of each variable against itself.

# Calculating linear correlation
res_cor_R2=cor(df_exp)[1,2]^2
sprintf("R2: %s", round(res_cor_R2,2))

# now computing the MIC metric
res_mine=mine(df_exp)
sprintf("MIC: %s", res_mine$MIC[1,2])

```
**MIC** value goes from 0 to 1. Being 0 no correlation and 1 highest correlation. The intepretation is the same as the R squared.

<br>

#### Analysis 

The `MIC=1` indicates there is a perfect correlation between the two variables. If we were doing **feature engineering** this variable should be included.

Further than a simple correlation, what the MIC says is: "Hey these two variables show a functional relationship". 

In machine learning terms (and oversimplifying): "variable `y` is dependant of variable `x` and a function -that we don't know which one- can be found model the relationship."

This is tricky, because that relationship was effectively created based on a function, an exponential one.

But let's continue with other examples...

<br>

#### Adding noise

Noise is an undesire signal adding to the original one. In machine learning noise helps the model to get confuse. Concretly: two identical input cases -for example customers- have different outcomes -one buy and the other doesn't-.

Now we are going to add some noise creating the `y_noise_1` variable.

```{r}
df_exp$y_noise_1=jitter(df_exp$y, factor = 1000, amount = NULL)
ggplot(df_exp, aes(x=x, y=y_noise_1)) + geom_line()
```

Calculating againg the correlation and MIC, printing in both cases the complete matrix, which shows the correlation/MIC metric of each input variable against all the others including themself.

```{r}
res_R2=cor(df_exp)^2
res_mine_2=mine(df_exp)
res_R2
res_mine_2$MIC

```

Adding noise to the data decreases the MIC value from 1 to 0.7226365 (-27%), and this is great!

R2 also decreased but just a little bit, from 0.3899148 to 0.3866319 (-0.8%). 

**Conclusion:** MIC reflects a noisy relationship much better than R2. Due to the nature of noise, it is very common to find it on a regular basis.


<br>

#### Measuring non-linearity


```{r, eval=FALSE}

round(res_mine_2$MICR2,3)
round(res_mine_2$MIC-res_R2,3)

```

<br>

#### Measuring periodicity

From ref [1]: _MAS is useful, for example, for detecting periodic relationships with unknown frequencies that vary over time_




<br>

#### A really important issue

Further than correlation, MIC measures if there are a _functional relationship_. In this case the variable `y` is generated directly from a negative exponential function. In other words, a high MIC indicates that a function can generate the variable thus is likely to find a predictive model. 

<br>

#### Other metrics 

A high value in MAS metric indicates there is a periodic relationship between two variables. Useful when analyzing time series.

#### Just MINE covers this?

We are going to cover only MINE suite, but there are other algortihms related to <a href="http://www.scholarpedia.org/article/Mutual_information" target="blank">mutual information</a>. In R some of the packages are: <a href="https://cran.r-project.org/web/packages/entropy/entropy.pdf" target="blank">entropy</a> and <a href="https://artax.karlin.mff.cuni.cz/r-help/library/infotheo/html/mutinformation.html" target="blank">infotheo</a>.



#### References

[1] Original paper MINE: <a href="http://science.sciencemag.org/content/334/6062/1518.full?ijkey=cRCIlh2G7AjiA&keytype=ref&siteid=sci" target="blank">Detecting Novel Associations in Large Data Sets</a>.


