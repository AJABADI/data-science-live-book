Correlation and relationship
=====

### Overview


### Linear correlation (classical)

It retrieves the well known `R statistic` (or Pearson coefficient), which measures **linear** correlation  for all numeric variables _(skipping the string ones)_.


```{r ,results="hide", echo=FALSE}
library(knitr)
#opts_knit$set(base.dir = "selecting_best_variables")
```


```{r lib, message=F, results="hide"}
## Loading needed libraries
library(funModeling)
library(minerva)
library(ggplot2)
library(dplyr)
library(reshape2)
options(scipen=999)

data(heart_disease)
```

```{r}
correlation_table(data=heart_disease, str_target="has_heart_disease")
```

`R statistic` goes from `1` _positive correlation_ to `-1` _negative correlation_. A value around `0` implies no correlation.
Squaring this number returns the `R squared` statistic (aka `R2`), which goes from `0` _no correlation_ to `1` _high correlation_. 

#### R2 bias problem

**R statistic is highly influenced by outliers and non-linear relationships.**

Outliers can be treated with `prep_outliers` function, present in this package.

Take a look at the **Anscombe's quartet**. These 4 relationships are quite different, but all of them have the same R2: 0.816.


```{r, out.width = 400, fig.retina = NULL, echo=F}
library(knitr)
include_graphics("anscombe_quartet.png")
```



Last plot, and more info about correlation can be found at: [Correlation and dependence](https://en.wikipedia.org/wiki/Correlation_and_dependence)

<br>

### Correlation based on Information Theory

This relationships can be measure better with <a href="https://en.wikipedia.org/wiki/Information_theory">Information Theory</a> conepts. One of the many algortihms to measure correlation based on this is: **MINE**, acronym for: Maximal Information-based nonparametric exploration.

The implementation in R can be found in <a href="https://cran.r-project.org/web/packages/minerva/index.html">minerva</a> package.


<br>

#### An example in R: Finding reliable variables

```{r, message=FALSE}

x=seq(0, 20, length.out=500)
df_exp=data.frame(x=x, y=dexp(x, rate=0.65))
ggplot(df_exp, aes(x=x, y=y)) + geom_line(color='steelblue') + theme_minimal()

# position [1,2] contains the correlation of both variables, excluding the correlation measure of each variable against itself.

# Calculating linear correlation
res_cor_R2=cor(df_exp)[1,2]^2
sprintf("R2: %s", round(res_cor_R2,2))

# now computing the MIC metric
res_mine=mine(df_exp)
sprintf("MIC: %s", res_mine$MIC[1,2])

# Important: If you recieve this message `Error: Missing values present in input variable 'x'. Consider using use = 'pairwise.complete.obs'.` is because data has missing values.


```
**MIC** value goes from 0 to 1. Being 0 no correlation and 1 highest correlation. The intepretation is the same as the R squared.

<br>

#### Analysis 

The `MIC=1` indicates there is a perfect correlation between the two variables. If we were doing **feature engineering** this variable should be included.

Further than a simple correlation, what the MIC says is: "Hey these two variables show a functional relationship". 

In machine learning terms (and oversimplifying): "variable `y` is dependant of variable `x` and a function -that we don't know which one- can be found model the relationship."

This is tricky, because that relationship was effectively created based on a function, an exponential one.

But let's continue with other examples...

<br>

### Adding noise

Noise is an undesire signal adding to the original one. In machine learning noise helps the model to get confuse. Concretly: two identical input cases -for example customers- have different outcomes -one buy and the other doesn't-.

Now we are going to add some noise creating the `y_noise_1` variable.

```{r}
df_exp$y_noise_1=jitter(df_exp$y, factor = 1000, amount = NULL)
ggplot(df_exp, aes(x=x, y=y_noise_1)) + geom_line(color='steelblue') + theme_minimal()
```

Calculating againg the correlation and MIC, printing in both cases the complete matrix, which shows the correlation/MIC metric of each input variable against all the others including themself.

```{r}
## calculating R squared
res_R2=cor(df_exp)^2
res_R2

## calculating mine
res_mine_2=mine(df_exp)

## MIC 
res_mine_2$MIC

```

Adding noise to the data decreases the MIC value from 1 to 0.7226365 (-27%), and this is great!

R2 also decreased but just a little bit, from 0.3899148 to 0.3866319 (-0.8%). 

**Conclusion:** MIC reflects a noisy relationship much better than R2 and it's helpful to find correlated associations. 

**In practice:** To generate data based on a function is only for teaching purposes. But the concept of noisy in variables is quite common in _almost_ **every data set**, no matter its source. Machine learning models deal with this noise, by approaching to the _real_ shape of data.

It's quite useful to use the MIC measure to get a sense about the information present in a relationship between two variables.

<br>

### Measuring non-linearity (MICR2)

`mine` function returns several metrics, we checked only **MIC**, but due to the nature of the algorithm (you can check the original paper at ref. [1]), it computes more interesting indicators. Check them all by inspecting `res_mine_2` object.

One of them is `MICR2`, used as a measured of **non-linearity**. It is calculated by doing the: MIC - R2. Since R2 measures the linearity, a high `MICR2` would indicate a non-linear relationship.

We can check it by calculating the MICR2 manually, following two matrix returns the same result:

```{r, eval=FALSE}
# MIC r2: non-linearity metric
round(res_mine_2$MICR2, 3)
# calculating MIC r2 manually
round(res_mine_2$MIC-res_R2, 3)
```

Non-linear relationships are harder to modelize, even more using a linear algorithm like decision trees or linear regression. 

Imagine we need to explain the relationship to other person, we'll need more details to explain it. It's easier to say: _"A increases as B increases and the ratio is always 3x"_ (if A=1 then B=3, linear); in comparisson to: _"A increases as B increases, but A is almost 0 until B reaches the value 10, then A raises to 300; and when B reaches 15, A goes to 1000."_

```{r, message=FALSE}
## creating data exampe
df_example=data.frame(x=df_exp$x, y_exp=df_exp$y, y_linear=3*df_exp$x+2)

## getting mine metrics
res_mine_3=mine(df_example)

## generating labels to print the results
results_linear=sprintf("MIC: %s \n MICR2 (non-linearity): %s", res_mine_3$MIC[1,3],round(res_mine_3$MICR2[1,3],2))

results_exp=sprintf("MIC: %s \n MICR2 (non-linearity): %s", res_mine_3$MIC[1,2],round(res_mine_3$MICR2[1,2],4))

## Plotting results ##
library(gridExtra)
## Creating plot exponential variable
p_exp=ggplot(df_example, aes(x=x, y=y_exp)) + geom_line(color='steelblue') + annotate("text", x = 11, y =0.4, label = results_exp) + theme_minimal()

## Creating plot linear variable
p_linear=ggplot(df_example, aes(x=x, y=y_linear)) + geom_line(color='steelblue') + annotate("text", x = 8, y = 55, label = results_linear) + theme_minimal()
grid.arrange(p_exp,p_linear,ncol=2)
```

<br>

Both plots shows a perfect correlation (or relationship), holding a MIC=1.
Regarding non-linearity, MICR2 behaves as expected, in `y_exp`=0.6101, and in `y_linear`=0. 

This point is important since the **MIC behaves equals to R2 does in linear relationships**, plus it adapts quite well to **non-linear** relationships as we saw before, retrieving an specific score metric (`MICR2`) to profile the relationship. 

<br>

### Measuring non-monotonicity in time series (MAS)

MINE can also help us to profile time series regarding its non-monotonicity with **MAS** (maximun asimetry score).

A monotonic serie is such it never changes its tendency, it always goes up or down. More on this on ref [3].

Following example simualtes two time series, one not-monotonic `y_1` and the other monotonic `y_2`.

```{r}
# creating sample data (simulating time series)
time_x=sort(runif(n=1000, min=0, max=1))
y_1=4*(time_x-0.5)^2
y_2=4*(time_x-0.5)^3

# Calculating MAS for both series
mas_y1=round(mine(time_x,y_1)$MAS,2)
mas_y2=mine(time_x,y_2)$MAS

## Putting all together
df_mono=data.frame(time_x=time_x, y_1=y_1, y_2=y_2)

## Plotting
p_y_1=ggplot(df_mono, aes(x=time_x, y=y_1)) + geom_line(color='steelblue') + theme_minimal()  + annotate("text", x = 0.45, y =0.75, label = sprintf("MAS=%s (goes down \n and up => not-monotonic)", mas_y1))

p_y_2=ggplot(df_mono, aes(x=time_x, y=y_2)) + geom_line(color='steelblue') + theme_minimal() + annotate("text", x = 0.43, y =0.35, label = sprintf("MAS=%s (goes up => monotonic)", mas_y2))

grid.arrange(p_y_1,p_y_2,ncol=2)

```

<br>

#### A more real example

Consider the following case which contains three time series: `y1`, `y2` and `y3`. They can be profilied in terms of its non-monotonicty, or overall growth trend.

```{r}
## reading data
df_time_series=read.delim(file="df_time.txt")

## converting to long format so they can be plotted
df_time_series_long=melt(df_time_series, id="time")

## Plotting
plot_time_series=ggplot(data=df_time_series_long,
       aes(x=time, y=value, colour=variable)) +
       geom_line() + theme_minimal()  + scale_color_brewer(palette="Set2")

plot_time_series
```

```{r}
# Calculating and printing MAS values for time series data
mine_ts=mine(df_time_series)
mine_ts$MAS 
```

<br>

We need to look at `time` column, so we've got the MAS value of each series regarding the time.
`y2` is the most not-monotonic series, and it can be confirmed by looking at it. It seems to be always up.

**MAS summary:**

* MAS ~ 0 indicates monotonic function
* MAS ~ 1 indicates non-monotonic function (_always_ up or down)


<br>

### Correlation between time series

MIC metric can also measure the **correlation in time series**, it is not a general purpose tool but can be helpful to quickly compare different series.

This section is based on the same data we used in MAS example.

```{r}
## printing again the 3-time series
plot_time_series
# Printing MIC values
mine_ts$MIC
```
<br>

Now we need to look at `y1` column. According to MIC measure, we can confirm the same that it's shown in last plot: 

`y1` is more similar to `y3` (MIC=0.709) than what is `y2` (MIC=0.61). 

#### Going further: Dynamic Time Wrapping

MIC will not be helpful for more complex esenarios having time series which vary in speed, you would use <a href="https://en.wikipedia.org/wiki/Dynamic_time_warping target="blank">dynamic time wrapping</a> technique.

Let's use an image to catch up the concept visually:

<img src="dynamic_time_wrapping.png" alt="time series clustering with dynamic time wrapping" width="350px">

_Image source: Ref [4]._

A nice implementation in R: <a href="http://dtw.r-forge.r-project.org/">dtw package</a> 

Finding correlations between time series is other way of performing **time series clustering**.

<br>

### Correlation on categorical (and binary) variables

<br>

### A really important issue

Further than correlation, MIC measures if there are a _functional relationship_. In this case the variable `y` is generated directly from a negative exponential function. In other words, a high MIC indicates that a function can generate the variable thus is likely to find a predictive model. 

<br>


### Just MINE covers this?

We are going to cover only MINE suite, but there are other algortihms related to <a href="http://www.scholarpedia.org/article/Mutual_information" target="blank">mutual information</a>. In R some of the packages are: <a href="https://cran.r-project.org/web/packages/entropy/entropy.pdf" target="blank">entropy</a> and <a href="https://artax.karlin.mff.cuni.cz/r-help/library/infotheo/html/mutinformation.html" target="blank">infotheo</a>.

help(mine) and run examples.

<br>

### References

* [1] Original MINE paper: <a href="http://science.sciencemag.org/content/334/6062/1518.full?ijkey=cRCIlh2G7AjiA&keytype=ref&siteid=sci" target="blank">Detecting Novel Associations in Large Data Sets</a>.
* [2] Some uses and explanations of MINE measurments in clinical data <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3561932/" target="blank">Characterizing Non-Linear Dependencies in clinical data</a>
* [3] Wikipedia <a href="https://en.wikipedia.org/wiki/Monotonic_function" target="blank">Monotonic function</a>
* [4] Dynamic time wrapping <a hreg="https://izbicki.me/blog/converting-images-into-time-series-for-data-mining.html" target="blank">Converting images into time series for data mining</a>