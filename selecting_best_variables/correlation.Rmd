Correlation and Information
=====

### What is this about?

It retrieves the well known `R statistic` (or Pearson coefficient), which measures **linear** correlation  for all numeric variables _(skipping the string ones)_.


```{r ,results="hide", echo=FALSE}
library(knitr)
#opts_knit$set(base.dir = "selecting_best_variables")
```


```{r lib, results="hide"}
## Loading funModeling !
suppressMessages(library(funModeling))
data(heart_disease)
```

```{r}
correlation_table(data=heart_disease, str_target="has_heart_disease")
```

`R statistic` goes from `1` _positive correlation_ to `-1` _negative correlation_. A value around `0` implies no correlation.
Squaring this number returns the `R squared` statistic (aka `R2`), which goes from `0` _no correlation_ to `1` _high correlation_. 

#### R2 bias problem

**R statistic is highly influenced by outliers and non-linear relationships.**

Outliers can be treated with `prep_outliers` function, present in this package.

Take a look at the **Anscombe's quartet**. These 4 relationships are quite different, but all of them have the same R2: 0.816.


```{r, out.width = 400, fig.retina = NULL, echo=F}
library(knitr)
include_graphics("anscombe_quartet.png")
```



Last plot, and more info about correlation can be found at: [Correlation and dependence](https://en.wikipedia.org/wiki/Correlation_and_dependence)

<br>

### Measuring non-linear relationships

This relationships can be measure better with <a href="https://en.wikipedia.org/wiki/Information_theory">Information Theory</a> conepts. One of the many algortihms to measure correlation based on this is: **MINE**, acronym for: Maximal Information-based nonparametric exploration.

The implementation in R can be found in <a href="https://cran.r-project.org/web/packages/minerva/index.html">minerva</a> package.

MIC statistic from minerva package measure 

```{r}
library(funModeling)
library(minerva)
library(ggplot2)
options(scipen=999)

# Important: If you recieve this message `Error: Missing values present in input variable 'x'. Consider using use = 'pairwise.complete.obs'.` is because data has missing values.

x=seq(0, 20, length.out=500)
df_exp=data.frame(x=x, y=dexp(x, rate=0.65))
ggplot(df_exp, aes(x=x, y=y)) + geom_line()

# position [1,2] contains the correlation of both variables, excluding the correlation measure of each variable against itself.

# Calculating linear correlation
res_cor_R2=cor(df_exp)[1,2]^2
sprintf("R2: %s", round(res_cor_R2,2))

# now computing the MIC metric
res_mine=mine(df_exp)
sprintf("MIC: %s", res_mine$MIC[1,2])

```
**MIC** value goes from 0 to 1. Being 0 no correlation and 1 highest correlation. The intepretation is the same as the R squared.

<br>

#### Analysis 

The MIC=1 indicates there is a perfect correlation between the two variables. We knew before hand that it was this way. This is a good variable to add to the predictive model. 



<br>

#### Adding noise

Noise is an undesire signal adding to the original one. In machine learning noise helps the model to get confuse. Concretly: two input cases -for example customers- have different outcomes.

Now we are going to add some noise creating the `y_noise_1` variable.

```{r}
df_exp$y_noise_1=jitter(df_exp$y, factor = 1000, amount = NULL)
ggplot(df_exp, aes(x=x, y=y_noise_1)) + geom_line()
```

Calculating againg the correlation and MIC, printing in both cases the complete matrix.

```{r}
res_R2=cor(df_exp)^2
res_mine_2=mine(df_exp)
res_R2
res_mine_2$MIC

```

Adding noise to the data decreases the MIC value from 1 to 0.7226365 (-27%), and this is great!

R2 also decreased but just a little bit, from 0.3899148 to 0.3866319 (-0.8%). 


**Conclusion:** MIC metric (based on Information Theory) reflects better than R2 the noise in variables. A situtation that occurs in almost any data set.

<br>

#### Measuring non-linearity


```{r, eval=FALSE}

a=cor(dat)[1,2]
b=a*a
res$MIC[1,2]-b
sprintf("Non-linear correlation (MIC): %s", res$MIC[1,2])
```

<br>

#### Measuring periodicity






<br>

#### A really important issue

Further than correlation, MIC measures if there are a _functional relationship_. In this case the variable `y` is generated directly from a negative exponential function. In other words, a high MIC indicates that a function can generate the variable thus is likely to find a predictive model. 

<br>

#### Other metrics 

A high value in MAS metric indicates there is a periodic relationship between two variables. Useful when analyzing time series.


#### References

<a href="http://science.sciencemag.org/content/334/6062/1518.full?ijkey=cRCIlh2G7AjiA&keytype=ref&siteid=sci" target="blank">Detecting Novel Associations in Large Data Sets</a>.


