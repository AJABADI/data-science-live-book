```{r ,results="hide", echo=FALSE}
library(knitr)
knitr::opts_chunk$set(out.width="600px", dpi=200)
``` 

# Analysis, Handling, and Imputation of Missing Data

## What is this about?

The analysis of missing values is the estimation of emptiness itself. Missing values present an obstacle to creating predictive models, cluster analyses, reports, etc.

In this chapter, we discuss the concept and treatment of empty values. We will perform analyses using different approaches and interpret the different results. 

Hopefully, after studying the whole chapter, the reader will understand key concepts of dealing with missing values and pursue better approaches than the ones proposed here.

<br>

**What are we going to review in this chapter?**

* What is the concept of an empty value?
* When to exclude rows or columns.
* Analysis and profiling of missing values.
* Transforming and imputing numeric and categorical variables.
* Imputing values: from easy to more complex approaches.

These will be exemplified using a practical approach in R. The code intends to be generic enough to apply to your projects üôÇ.

<br>


---



## When the empty value represents information

Empty values are also known as ‚ÄúNULL‚Äù in databases, `NA` in R, or just the ‚Äúempty‚Äù string in spreadsheet programs. It can also be represented by some number like: `0`, `-1` or `‚àí999`. 

For example, imagine a travel agency that joins two tables, one of persons and another of countries. The result shows the number of travels per person: 

```{r, echo=FALSE}
df_travel=data.frame(person=c("Fotero", "Herno", "Mamarul"), South_Africa=c(1, NA, 34), Brazil=c(5,NA,40), Costa_Rica=c(5,NA,NA), stringsAsFactors = F)
df_travel
```

In this result, `Mamarul` traveled to `South Africa` `34` times.

_What does the `NA` (or NULL) value represent?_

In this case, NA should be replaced by 0, indicating zero travels in that person‚Äìcountry intersection. After the conversion, the table is ready to be used.

**Example: Replace all NA values by 0**

```{r}
# Making a copy
df_travel_2=df_travel
  
# Replacing all NA values with 0
df_travel_2[is.na(df_travel_2)]=0
df_travel_2
```

The last example transforms **all** `NA` values into `0`. However, in other scenarios, this transformation may not apply to all columns.

**Example: Replace NA values by 0 only in certain columns**

It is probably the most common scenario to replace NA by some value‚Äîzero in this case‚Äîonly to some columns. We define a vector containing all the variables to replace and then we call on the `mutate_at` function from the `dplyr` package.

```{r, message=FALSE}
library(dplyr) # vers 0.7.1

# Replacing NA values with 0 only in selected columns
vars_to_replace=c("Brazil", "Costa_Rica")

df_travel_3=df_travel %>% mutate_at(.vars=vars_to_replace, .funs = funs(ifelse(is.na(.), 0, .)))

df_travel_3
```
  
Keep at hand the last function as it is very common to face the situation of applying a specified function to a subset of variables and returning the transformed and the non-transformed variables in the same dataset.

Let's go to a more complex example.

<br>


## When the empty value is an empty value

Other times, to have an empty value is correct, it‚Äôs expressing the absence of something. We need to treat them to use the table. Many predictive models don't handle input tables with missing values.

In some cases, a variable is measured _after_ a period of time, so we have data from this point on and NA before. 

Sometimes there are random cases, like a machine that fails to collect the data or a user who forgot to complete some field in a form, among others.

One important question arises: _What to do?!_ `r emo::ji("scream")`

The following recommendations are just that, recommendations. You can try different approaches to discover the best strategy for the data you are analyzing. **There is no ‚Äúone-size-fits-all‚Äù**.

<br>

---

## Excluding the entire row

If at least one column has an `NA` value, then exclude the row.

A fast and easy method, right? It's recommended when the number of rows is _low_. But how low is low? That's up to you. Ten cases in 1,000 of rows _may not_ have a huge impact unless those 10 cases are related to the prediction of an anomaly; in this instance, it represents information. We pointed out this issue in <a href="http://livebook.datascienceheroes.com/data_preparation/high_cardinality_predictive_modeling.html#case-1-reducing-by-re-categorizing-less-representative-values" target="blank">case 1: reducing by re-categorizing less representative values.</a>

<br>

**Example in R:**

Let's inspect the `heart_disease` dataset with the `df_status` function, where one of its primary objectives is to help us with these kinds of decisions. 

```{r, message=FALSE}
library(dplyr) # vers 0.7.1
library(funModeling) # vers 1.6.4
df_status(heart_disease, print_results = F) %>% select(variable, q_na, p_na) %>% arrange(-q_na)
```

`q_na` indicates the quantity of `NA` values and `p_na` is the percentage. Full info about `df_status` can be found in <a href="http://livebook.datascienceheroes.com/data_preparation/profiling.html" target="blank">Profiling chapter</a>.

Two variables have 4 and 2 rows with `NA` values, so we exclude these rows:

```{r}
# na.omit returns the same data frame having excluded all rows containing at least one NA value
heart_disease_clean=na.omit(heart_disease)
nrow(heart_disease) # number of rows before exclusion
nrow(heart_disease_clean) # number of rows after exclusion
```

After the exclusion, six rows out of 303 were eliminated. This approach seems suitable for this dataset.

However, there are other scenarios in which almost all cases are empty values, thus exclusion will delete the entire dataset!

<br>

---

## Excluding the column

Similar to the last case, we exclude the column. If we apply the same reasoning and if the deletion is about a _few_ columns and the remaining ones provide a reliable final result, then it may be acceptable. 

**Example in R:**

These exclusions are easily handled with the `df_status` function. The following code will keep all variable names for which the percentage of `NA` values are higher than 0.

```{r}
## Getting variable names with NA values
vars_to_exclude=df_status(heart_disease, print_results = F) %>% filter(p_na > 0) %>% .$variable

## Checking variables to exclude
vars_to_exclude

## Excluding variables from original dataset
heart_disease_clean_2=select(heart_disease, -one_of(vars_to_exclude))
```

<br>

---

## Treating empty values in categorical variables

We cover different perspectives to convert as well as treat empty values in nominal variables.

Data for the following example are derived from `web_navigation_data` which contains standard information regarding how users come to a particular web page. It contains the `source_page` (the page the visitor comes from), `landing_page` (first page visited), and `country`.

```{r}
# When reading example data, pay attention to the na.strings parameter
web_navigation_data=read.delim(file="https://raw.githubusercontent.com/pablo14/data-science-live-book/master/data_preparation/web_navigation_data.txt", sep="\t", header = T, stringsAsFactors=F, na.strings="")
```


**Profiling the data:**

```{r}
stat_nav_data=df_status(web_navigation_data)
```

The three variables have empty (`NA`) values. Almost half of the values in `source_page` are missing while the other two variables have 5% and 3% `NA`s.

### Case A: Convert the empty value into a string

In categorical or nominal variables, the quickest treatment is to convert the empty value into the string `unknown`. Therefore, the machine learning model will handle the "empty" values as another category. Think about it like a rule: "If variable_X = unknown, then the outcome = yes".

Next, we propose two methods intended to cover typical scenarios.

**Example in R:**

```{r}
## Method 1: Converting just one variable and create a new variable
web_navigation_data_1=web_navigation_data # making a copy

# Creating a new variable
web_navigation_data_1$source_page_2=ifelse(is.na(web_navigation_data$source_page), "unknown_source", web_navigation_data$source_page)

## Method 2: It's a typical situation only to apply a function to specific variables and then return the original data frame
# First, we define the conversion function
convert_categ<-function(x)
{
  # If NA, then put ‚Äúunknown,‚Äù otherwise return the same value
  ifelse(is.na(x), "unknown", x)                                                                 
}                                 

# Wmagine we want to convert all variables with less than 6% NA values:
vars_to_process=filter(stat_nav_data, p_na<6) 

# Create the new data frame with the transformed variables
web_navigation_data_2=web_navigation_data %>% mutate_at(.vars=vars(vars_to_process$variable), .funs=funs(convert_categ))
```

Checking the results:

```{r}
df_status(web_navigation_data_2)
```

Note: To apply a function to certain columns is a very common task in any data project. More info about how to use it `mutate_at` from `dplyr`: https://stackoverflow.com/questions/27027347/mutate-each-summarise-each-in-dplyr-how-do-i-select-certain-columns-and-give

<br>

### Case B: Assign the most frequent category

The intuition behind this method is _to add more of the same as to not affect the variable_. However,  sometimes it does. It will not have the same impact if the most common value appears 90% of the time than if it does 10%; that is, it depends on the distribution.

This technique is more suitable in a predictive model that is run on production and a new value for categorical variables. If the predictive model is robust, like **random forest** is, then it will throw the message: `New factor levels not present in the training data` where `factor level` is equal to `new category value`. 

This book covered this point in <a href="http://livebook.datascienceheroes.com/data_preparation/high_cardinality_predictive_modeling.html"
 target="blank">High Cardinality Variable in Predictive Modeling</a>.

As you can see, the situation is not the same if we are building a predictive model to go live or doing an ad hoc report.


<br>

### Case C: Exclude some columns and transform others

The easy case is if the column contains, let's say, 50% `NA` cases, making it highly likely not to be reliable. 

In the case we saw before, `source_page` has more than half of the values empty. We could exclude this variable and transform ‚Äîas we did‚Äî the remaining two.

The example is prepared to be generic:

```{r}
# Setting the threshold
threshold_to_exclude=50 # 50 Represents 50%
vars_to_exclude=filter(stat_nav_data, p_na>=threshold_to_exclude) 
vars_to_keep=filter(stat_nav_data, p_na<threshold_to_exclude) 

# Finally...
vars_to_exclude$variable
vars_to_keep$variable

# Next line will exclude variables above the threshold and transform the remaining ones
web_navigation_data_3=select(web_navigation_data, -one_of(vars_to_exclude$variable)) %>%
  mutate_at(.vars=vars_to_keep$variable, .funs=funs(convert_categ))

# Checking there are no NA values and the variable above the NA threshold has disappeared
df_status(web_navigation_data_3)

```

<br>  

### Summing-up 

What if the data contain 40% `NA` values? It depends on the objective of the analysis and the nature of the data. 

The important point here is to ‚Äúsave‚Äù the variable so we can use it. Finding many variables with missing values is common. It may be that those _incomplete variables_ carry useful predictive information when they have a value, therefore, we need to treat them and then build a predictive model. 

However, we need to minimize the bias we are introducing because the missing value is a value that "is not there".

* When doing a report, the suggestion is to replace `NA` by the string `empty`,
* When doing a predictive model that is running live, assign the most repetitive category. 

<br>

---

## Is there any pattern in missing values?

First, load the example movie data and do a quick profile.

```{r}
# Lock5Data contains many data frames to practice
# install.packages("Lock5Data")
library(Lock5Data)

# loading data
data("HollywoodMovies2011")

# profiling
df_status(HollywoodMovies2011)
```

Let's take a look at the values present in the `p_na` column. There is a pattern in the missing values: four variables have 1.47% NA values and another four have around 11.7%. In this case, we are not able to check the data source; however, it is a good idea to check if those cases have a common issue.

<br> 

---

## Treating missing values in numerical variables

We approached this point at the beginning of the chapter by converting all `NA` values to 0.

One solution is to replace the empty by the mean, median, or other criteria. However,  we have to be aware of the change in the distribution that this creates.

If we see that the variable seems to be correlated when it's not empty (same as categorical), the an alternative method is to create bins, also known as "buckets" or "segments", thereby converting it to categorical.


### Method 1: Converting into categorical

The function `equal_freq` splits the variable into the desired bins:

```{r na_missing_values_analysis, fig.height=3, fig.width=6, echo=FALSE, out.width = "400px"}
HollywoodMovies2011$TheatersOpenWeek_2=equal_freq(HollywoodMovies2011$TheatersOpenWeek, n_bins=5)

freq(HollywoodMovies2011, "TheatersOpenWeek_2")
```

As we can see, `TheatersOpenWeek_2` contains five buckets of 24 cases each, where each represents 20% of total cases. But, the `NA` values are still there.

Finally, we have to convert the NA into the string `empty`.

```{r, fig.height=3, fig.width=6, echo=FALSE, out.width = "400px", results=FALSE}
HollywoodMovies2011$TheatersOpenWeek_2=as.character(HollywoodMovies2011$TheatersOpenWeek_2)
HollywoodMovies2011$TheatersOpenWeek_2=ifelse(is.na(HollywoodMovies2011$TheatersOpenWeek_2), "empty", HollywoodMovies2011$TheatersOpenWeek_2)

freq(HollywoodMovies2011, "TheatersOpenWeek_2")
```

And that's it: the variable is ready to be used. 

**Custom cuts**:
If we want to use custom bucket sizes instead of the ones provided by equal frequency, then we can do the following:

```{r}
options(scipen=999) # disabling scientific notation in current R session

# Creating custom buckets, with limits in 1,000, 2,300, and a max of 4,100. Values above 4,100 will be assigned to NA.

HollywoodMovies2011$TheatersOpenWeek_3=cut(HollywoodMovies2011$TheatersOpenWeek, breaks = c(0, 1000, 2300, 4100), include.lowest = T, dig.lab = 10)

freq(HollywoodMovies2011, "TheatersOpenWeek_3", plot = F)
```


It should be noted that **equal frequency binning** tends to be more robust than the equal distance that splits the variable, which is based on taking the min and max, and the distance between each segment, regardless how many cases fall into each bucket. 

The equal frequency puts the outliers values in the first or last bin as appropriate. Normal values can range from 3 to 20 buckets. A higher number of buckets tend to be noisier. For more info, check the <a href="http://livebook.datascienceheroes.com/selecting_best_variables/cross_plot.html" target="blank">`cross_plot`</a> chapter function.

<br>

### Method 2: Filling the NA with some value

As with categorical variables, we can replace values by a number such as the mean or the median. 

In this case, we'll replace `NA` by the average and plot the before and after results side-by-side.

```{r, fig.height=3, fig.width=6, warning=FALSE, message=FALSE}
# Filling all NA values with the mean of the variable
HollywoodMovies2011$TheatersOpenWeek_mean=ifelse(is.na(HollywoodMovies2011$TheatersOpenWeek), mean(HollywoodMovies2011$TheatersOpenWeek, na.rm = T), HollywoodMovies2011$TheatersOpenWeek)

# Plotting original variable
p1=ggplot(HollywoodMovies2011, aes(x=TheatersOpenWeek)) + geom_histogram(colour="black", fill="white") + ylim(0, 30)
 
# Plotting transformed variable
p2=ggplot(HollywoodMovies2011, aes(x=TheatersOpenWeek_mean)) + geom_histogram(colour="black", fill="white") + ylim(0, 30)

# Putting the plots side-by-side 
library(gridExtra)
grid.arrange(p1, p2, ncol=2)
  
```

We can see a peak at `2828`, which is a product of the transformation. This introduces a bias around this point. If we are predicting some event, then it would be safer not to have some special event around this value. 

For example, if we are predicting a binary event and the least representative event is correlated with having a mean of `3000` in `TheatersOpenWeek`, then the odds of having a higher **False Positive rate** may be higher. Again, the link to the <a href="http://livebook.datascienceheroes.com/data_preparation/high_cardinality_predictive_modeling.html"
 target="blank">High Cardinality Variable in Predictive Modeling</a> chapter.

As an extra comment regarding the last visualization, it was important to set the y-axis maximum to 30 to make the plots comparable.

As you can see, there is an inter-relationship between all concepts `r emo::ji("wink")`.

<br>
  
### Picking up the right value to fill 

The last example replaced the `NA` with the mean, but how about other values? It depends on the distribution of the variable.

The variable we used (`TheatersOpenWeek`) seems normally distributed, which is the reason we used the mean. However, if the variable is more skewed, then another metric probably would be more suitable; for example, the median is less sensitive to outliers. 

<br>

---

## Advanced imputation methods

Now we are going to do a quick review of more sophisticated imputation methods in which we create a predictive model, with all that it implies.

<br>

### Method 1: Using random forest (missForest)

The <a href="https://cran.r-project.org/web/packages/missForest/missForest.pdf" target="target">`missForest` package</a> its functionality its based on running several random forests in order to complete each missing value in an iterative process, handling both categorical and numerical variables at the same time.

Regardless of missing value imputation, the random forest model has one of the best performances of many different kinds of data.
In next example, we will complete the `HollywoodMovies2011` data we were working with before. These data contain `NA` values in both numerical and categorical variables.

```{r, message=FALSE}
# install.packages("missForest")
library(missForest)

# Copying the data
df_holly=HollywoodMovies2011

# We will introduce 15% more NA values in TheatersOpenWeek_3 to produce a better example. The function prodNA in missForest will help us.
set.seed(31415) # to get always the same number of NA values...
df_holly$TheatersOpenWeek_3=prodNA(select(df_holly, TheatersOpenWeek_3), 0.15)[,1]

# missForest fails if it has any character variable, so we convert the only character into the factor:
df_holly$TheatersOpenWeek_2=as.factor(df_holly$TheatersOpenWeek_2)

# Excluding the .id column
df_holly=select(df_holly, -Movie)

# Now the magic! Imputing the data frame
# xmis parameter=the data with missing values
imputation_res=missForest(xmis = df_holly)

# Final imputed data frame
df_imputed=imputation_res$ximp
```

Now it's time to compare the distributions of some of the imputed variables. Hopefully, they will look similar on a visual analysis. 


```{r how_to_impute_missing_values, warning=FALSE, message=FALSE,  out.width="500px", fig.height=4, fig.width=5}
# Creating another imputation based on na.rougfix from the random forest package
df_rough=na.roughfix(df_holly)

# Compare distributions before and after imputation
df_holly$imputation="original"
df_rough$imputation="na.roughfix"
df_imputed$imputation="missForest"

# Putting the two data frames in only one, but split by is_imputed variable
df_all=rbind(df_holly, df_imputed, df_rough)

# Converting to factor for using in a plot
df_all$imputation=factor(df_all$imputation, levels=unique(df_all$imputation))

# Plotting
ggplot(df_all, aes(TheatersOpenWeek, colour=imputation)) + geom_density() + theme_minimal() + scale_colour_brewer(palette="Set2")
```


* The green curve shows the distribution after the imputation based on the `missForest` package.
* The orange shows the imputation method we discussed at the beginning, which replaces all `NA` by the median using the `na.roughfix` function in the `randomForest` package.
* The blue one shows the distribution without any imputation (of course, NA values are not displayed). 

**Analysis:**

Replacement of `NA` by the median tends to concentrate, as expected, all the values around 3000. On the other side, the imputation given by the missForest package provides a **more natural distribution** because it doesn't concentrate around a single value. That's why the peak around 3000 is lower than the original one.

The orange and blue look pretty similar!

If we want to take an analytical point of view, then we can run a statistical test to compare, for example, the means or the variance.
 
<br>

```{r missing_value_imputation, out.width="500px", fig.height=4, fig.width=7}
# An ugly hack to plot NA as a category
levels(df_all$TheatersOpenWeek_3)=c(levels(df_all$TheatersOpenWeek_3), "NA")
df_all$TheatersOpenWeek_3[is.na(df_all$TheatersOpenWeek_3)]="NA"

# Now the plot!
ggplot(df_all, aes(x = TheatersOpenWeek_3, fill = TheatersOpenWeek_3)) +
    geom_bar(na.rm=T) + facet_wrap(~imputation)  + geom_text(stat='count',aes(label=..count..),vjust=-1) + ylim(0, 125) + scale_fill_brewer(palette="Set2") + theme_minimal() + theme(axis.text.x=element_text(angle = 45, hjust = 0.7))


```

<br>


**Analysis:**

The original variable contains 31 `NA` values that were replaced using the mode (most frequent value) in `na.roughfix`, and with smoother and more reasonable criteria using `missForest`.

`missForest` added two rows in the category `[0, 1000]`, 1 in `[1000, 2300]`, and 32 in the `[2300, 4100]` category, whereas `na.roughfix` added only 35 to `[2300, 4100]`.


<br>

### Method 2: Using the MICE approach

**Advice:** For the very first approach to missing value imputation, this method is really complex `r emo::ji("fearful")`. 

MICE stands for "Multivariate Imputation by Chained Equations" also known as "Fully Conditional Specification". This book covers it due to its popularity.

MICE entails a complete framework to analyze and deal with missing values. It considers the interactions among **all variables** at the same time (multivariate and not just one) and bases its functionality on an **iterative** process that uses different predictive models to fill each variable. 

Internally, it fills variable A, based on B and C. Then, it fills B based on A and C (A is previously predicted) and the iteration continues. The name "chained equations" comes from the fact that we can specify the algorithm per variable to impute the cases.

This creates M replications of the original data with no missing values. _But why create M replications?_ 

In each replication, the decision of what value to insert in the _empty slot_ is based on the distribution. 

Many MICE demonstrations focus on validating the imputation and using the predictive models that support the package, which number only a few. This is great if we don't want to use other predictive models (random forest, gradient boosting machine, etc.), or a cross-validation technique (e.g., `caret`). 

The MICE technique puts the final result by setting a `pool()` function that averages the parameters (or betas) of the M predictive models providing facilities for measuring the variance due to missing values. 

Yes, one model per each generated data frame. Sounds like <a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" target="blank">bagging</a>, isn't it? But we don't have this possibility with the mentioned models. 

MICE has many functions to help us process and validate the filling results. But, to keep it very simple, we'll cover just a little part of it. The following example will focus on extracting a **data frame with no missing values ready to be used** with other programs or predictive models. 

**Example in R:**

This will impute data for the `nhanes` data frame coming in <a href="https://cran.r-project.org/web/packages/mice/mice.pdf" target="blank">mice package</a>. Let's check it:

```{r, message=FALSE}
# install.packages("mice")
library(mice)
df_status(nhanes)
```

Three variables have missing values. Let's fill them:

```{r}
# Default imputation creates five complete datasets
imp_data=mice(nhanes, m = 5, printFlag = FALSE)

# Get a final dataset containing the five imputed data frames, total rows=nrow(nhanes)*5
data_all=complete(imp_data, "long")

# data_all contains the same columns as nhanes plus two more: .id and .imp
# .id=row number from 1 to 25
# .imp=imputation data frame .id 1 to 5 (m parameter)
```

In the original data, `nhanes` has 25 rows and data_all contains 125 rows, which is the result of creating 5 (`m=5`) complete data frames of 25 rows each. 

Time to check the results:

```{r, fig.height=3, fig.width=6}
densityplot(imp_data)
```

Each red line shows the distribution of each imputed data frame and the blue one contains the original distribution. The idea behind this is that if they look similar, then the imputation followed the original distribution. 

For example, chl contains one imputed data frame; thus, only one red line containing two peaks around two values much higher than the original ones.  

The drawbacks are it is a slow process that may require some tuning to work. For example: `mice_hollywood=mice(HollywoodMovies2011, m=5)` will fail after some time processing it and it is a small data frame.

<a href="https://datascienceplus.com/handling-missing-data-with-mice-package-a-simple-approach/" target="blank"></a>


Original MICE paper: <a href="https://www.jstatsoft.org/article/view/v045i03" target="blank">Multivariate Imputation by Chained Equations in R.</a>

<br>

---

## Conclusions

After covering everything, we could ask: what is the best strategy? Well, it depends on how much we need to intervene in order to handle missing values.

A quick review of the strategies follows: 

A) Excluding the rows and columns with missing values. Only applicable if there is _a few_ rows (or columns) with missing values, **and** if the remaining data are enough to achieve the project goal. However, when we exclude rows with missing values and we build a predictive model that will run on production, when a **new case arrives** that contains missing values, we must assign a value to process these. 

B) The strategies of **converting numerical variables to categorical** and then creating the ‚Äúempty‚Äù value (also applicable to categorical variables), is the quickest option and we are introducing the missing value to the model so that it will consider the uncertainty. 

C) The **imputation methods** like the ones we covered with MICE and missForest are considerably more complex. With these methods, we introduce a **controlled-bias** so that we don't have to exclude any rows or columns. 

It's an art to find the correct balance between digging deeply into these transformations and keeping it simple. The invested time may not be reflected in the overall accuracy.

Regardless the method, it's quite important to analyze the impact of each decision. There is a lot of trial-and-error as well as exploratory data analysis leading up to the discovery of the most suitable method for your data and project.
 
<br>



