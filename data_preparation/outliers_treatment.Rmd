```{r ,results="hide", echo=FALSE}
library(knitr)
knitr::opts_chunk$set(out.width="400px", dpi=150, fig.width = 4, fig.height = 3)
knitr::opts_knit$set(base.dir = "data_preparation")
``` 

# Treatment of outliers

## What is this about?

The extreme values is a concept that, as well as many other topics in machine learning, it's not exclusive of this area. What it is an outlier today, may not be tomorrow. The boundaries between a normal and abnormal are fuzzy, to be in the extremes it's easy.

<br>
  
**What are we going to review in this chapter?**

<br>

## The intuition behind

For example, consider the followig distribution:
  
```{r dealing_with_outliers}
df_1=data.frame(var=round(10000*rbeta(1000,0.15,2.5)))

library(ggplot2)

ggplot(df_1, aes(var, fill=var)) + geom_histogram(bins=20) + theme_light()
```


The variable is skewed to the left, showing some outliers points on the right. We want to _deal with them_
(`r emo::ji("sunglasses")`). So the question appears: _Where to set the threshold?_ Based on intuition, it can be at highest 1% or we can analyze how much the `mean` change after removing top 1%.

Both cases could be right. In fact taking other number as the threshold (i.e. 2% or 0.1%), may be right too. Let's visualize them:

```{r dealing_with_outliers_4}
## Calculating the percentiles top 3% and top 1%
percentile_var=quantile(df_1$var, c(0.98, 0.99, 0.999), na.rm = T)
df_p=data.frame(value=percentile_var, percentile=c("a_98th", "b_99th", "c_99.9th"))

## Plotting the same distribution plus the percentiles
ggplot(df_1, aes(var)) + geom_histogram(bins=20) +
geom_vline(data=df_p, 
aes(xintercept=value, 
colour = percentile),
show.legend = TRUE, linetype="dashed") + theme_light()
```

To understand more about percentiles, please go to the _Profiling Appendix_.

Now, we'll keep with top 1% (percentile 99th), as the threshold to flag all the points after it as outliers.
  
```{r dealing_with_outliers_2, fig.height=3, fig.width=4, echo=FALSE}

## Plotting the same distribution plus the percentiles
ggplot(df_1, aes(var)) + geom_histogram(bins=20) +
  geom_rect(aes(xmin = df_p$value[2], xmax = Inf, ymin = -Inf, ymax = Inf), fill = "pink", alpha = 0.01)+
  geom_vline(data=df_p, 
             aes(xintercept=value[2], 
                 colour = percentile[2]),
             show.legend = F, linetype="dashed") + theme_light() 
```


A interesting conceptual point here is, when we define what **abnormal** is (or anomaly) is, the **normal concept emerges as its opposite**:


```{r dealing_with_outliers_3, fig.height=3, fig.width=4, echo=FALSE,tidy=TRUE}
## Plotting the same distribution plus the percentiles
ggplot(df_1, aes(var)) + geom_histogram(bins=20) +
  geom_rect(aes(xmin = -Inf, xmax = df_p$value[2], ymin = -Inf, ymax = Inf), fill = "lightgreen", alpha = 0.01)+
  geom_vline(data=df_p, 
             aes(xintercept=value[2], 
                 colour = percentile[2]),
             show.legend = F, linetype="dashed") + theme_light() 
```




Now keep with one The hard thing to do is to define where the normal and abnormal separate. There are several approaches to handle this, based on different criteria. By now this chapter is focused on the outlier treatment of bottom and top X%.

<br>

### Where is the boundary between hot and cold wheater? 

<img src="where_is_the_cutpoint.png" alt="Finding the cutpoint" width="300px">

Where can we put the threshold to stablish where the hot wheater begins? Or analyzing on the oppositive, when do the cold wheather end?

Near the Ecuator, probably a temperature around 10°C (50°F) is an extreme low value, but in the Antarctica, it's a beach day! 
`r emo::ji("snowman")` `r emo::ji("beach_umbrella")`

`r emo::ji("japanese_goblin")`: _"Oh! But that is taking an extreme example with two different locations!"_

No problem! like a fractal, let's zoom in to any city, the boundary when one start (and the other end) will not have one unique value to state the following: _"Ok, the hot wheather starts at 25.5°C (78°F)."_ 

It's relative.

However, it's quite easy to stand in the extremes, the uncertanty decreases to almost 0, i.e., a temperature of 60°C (140°F).

`r emo::ji("thinking")`: _"Ok. But how are these concepts related with machine learning?"_

We're exposing here the relativness in the decision when dealing with the labels (hot/cold) in a numeric variable (temperature). It can be thought for any other numeric, such as `income`, and the labels `normal`/`abnormal`.

To **understand the extremes** in the variable is one of the first task in **exploratory data analysis**. Then we can see what the normal values are. This is wide covered in the **Profiling** chapter.

When dealing with outliers, there are several methods to flag values as outliers. Just as we analyze the whather temperature, this flag is _relative_ and every method can be right. The quickest method is to treat as outliersthe top and bottom X%. Others more robust take into consideration the distribution variables using the quantiles (Tukey method) or how far the values are from 3 times the standard deviation.

The thresholds to spot outliers are not unique. Defining boundaries is one of the most common task in machine learning.

Not all questions have to have an answer, some of them just help us simply to think.

<br>

## The impact of outliers

### Model building

Some models such as random forest and gradient boosting machines tend to deal better with outliers, but some noise may affect the results anyway. The impact of outliers in these models are lower than others like linear regressions, logistic regressions, kmeans, decision trees.

One point that contributes to decrease the impact is both models create _many_ sub-models. If any of the model take one outlier as information, probably other of the sub-model won't. The error is cancelled. 

### Communicating results

If we need to report the variables used in the model, we'll end up removing outliers to not see an histogram with only one bar, and/or show not a biased mean. 

It's better to show a non-biased number than justifying the model _will handle_ extreme values.

### Types of outliers by data type
  
* **Numerical** `r emo::ji("straight_ruler")`: Like the one we saw before:

```{r numerical_outliers_1, fig.height=1.6, fig.width=2.8, echo=FALSE}

## Plotting the same distribution plus the percentiles
ggplot(df_1, aes(var)) + geom_histogram(bins=20) +
  geom_rect(aes(xmin = df_p$value[2], xmax = Inf, ymin = -Inf, ymax = Inf), fill = "pink", alpha = 0.01)+
  geom_vline(data=df_p, 
             aes(xintercept=value[2], 
                 colour = percentile[2]),
             show.legend = F, linetype="dashed") + theme_light() 
```


* **Categorical** `r emo::ji("bar_chart")`: Having a variable in which the dispersion of categories is quite high (high cardinallity). For example: postal code. More about dealing with outliers in categorical variable in the <a href="http://livebook.datascienceheroes.com/data_preparation/high_cardinality_descriptive_stats.html" target="blank">High Cardinality Variable in Descriptive Stats</a> chapter.

```{r categorical_outliers_1, echo=FALSE, message=FALSE}
library(funModeling)
library(dplyr)
data_country_sample=filter(data_country, country %in% c("France", "China", "Uruguay", "Peru", "Vietnam"))

freq(data_country_sample$country)
```

Countries `Peru` and `Vietnam` are the outliers here. Their share in data is less than 1%.

<br>

### Types of outliers by their dimensionsianlity

We saw the one dimension (1-D) so far, univariate outlier analysis. But we can consider two -or more- variables at a time.

For instance we have the following data set `df_hello_world` with two variables, `v1` and `v2`. Doing the same analysis as before:

```{r outlier_analysis, echo=FALSE}
## Creation data set
v1=c(rep("Argentina",50), rep("Uruguay",50), rep("Uruguay",30), "Argentina", "Argentina", "Argentina", "Argentina")
v2=c(rep("cat_A",50), rep("cat_B",50), rep("cat_A",30), "cat_B", "cat_A", "cat_A", "cat_A")
df_hello_world=data.frame(v1, v2)
```

```{r outlier_analysis, echo=FALSE}
freq(df_hello_world)
```

No one outlier so far, isn't that right?

Now we build a contigency table which tells us the distribution of both variables against each other:

```{r outlier_analysis, echo=FALSE}
## First, it is needed to crate the table object, for then, calculating the percentage per cell
tbl_hello_world=table(df_hello_world)

## Print the results in percentage per cell
round(100*prop.table(tbl_hello_world), 2)
```

Oh `r emo::ji("scream")`! The combination of `Argentina` and `cat_B` is _really low_ (0.75%) in comparisson with the other values. Less than 1% while the other intersections are above `22%`.

<br>

### Some thoughts...

The last examples, shown the _potential_ extreme values or outliers, are just exmaples to keep in mind when we are in touch with a new data set.

We mention the **1%** as a possible threshold to flag a value as an outlier. This value could be 0.5% or 3%, depending on the case.

Also the presence of these kind of outliers may not be a problem.


<br>


## Dealing with outliers using R

The `prep_outliers` function present in `funModeling` package can help us in this task. It can handle a single vector/variable, and several variables at a time (specifying the `str_input` parameter)

The core is:

* It supports three different methods (`method` parameter) to consider a value as an outlier: `bottom_top`, `tukey` and `hampel`.
* It works in two modes (`type` parameter), by setting an `NA` value or by _stopping the variable_ at a certain value. 


<br>

### How to detect outliers?

We have the following methods implemented in `prep_outliers` function. They retrieve different result so the user can select the one that most fits her/his needs.

#### Bottom and top values method

It considers an outlier based on the bottom and top X% values, based on the percentile. The values are commonly 0.5%, 1%, 1.5%, 3%, among others.

Setting the parameter `top_percent` in `0.01` will treat all values on the top 1%.

Same logic goes for the lowest values, setting parameter `bottom_percent` in 0.01 will flag as an outlier the lowest 1% of all values.

The internal function used is `quantile`, if we want to flag bottom and top 1%, we type:

```{r}
quantile(heart_disease$age, probs = c(0.01, 0.99), na.rm = T)
```
All below 35 and above 71 will be consider as outlier.

For more information about percentiles, check the chapter: "The magic of percentiles".

<br>

#### Tukey's method

It flags outliers considering the quartiles values, `Q1`, `Q2` and `Q3`. Where `Q1` is equivalent to percentile 25th, `Q2` equals to percentile 50th (also known as the median) and `Q3` is the percentile 75th.

The IQR, (aka. inter quartile range), comes from: `Q3`-`Q1`.

**The formula:**
* The bottom threshold is: `Q1 - 3*IQR`. All below is consider as an outlier.
* The top threshold is: `Q1 + 3*IQR`. All above is considered as an outlier.

The value `3` is to consider the "extreme" boundary detection. This method comes from the box plot, and there the multiplier is `1.5` (not `3`). This causes that a lot of more values are flagged as shown in the next image.

<img src="boxplot.png" alt="Boxplot, it shows the outliers using the 1.5 multiplier" width="300px">

The internal function used in `prep_outliers` to calculate the Tukey's boundary can be accessed:

```{r}
tukey_outlier(heart_disease$age)
```

It returns a two-value vector. And there we have the bottom and the top threshold. All below 9 and all above 100 will be considered as an outlier.

A nice visual and step-by-step example can be found in Ref. [1]. 

<br>

#### Hampel's method

**The formula:**
* The bottom threshold is: `median_value-3*mad_value`. All below is consider as an outlier.
* The top threshold is: `median_value+3*mad_value`. All above is considered as an outlier.


The internal function used in `prep_outliers` to calculate the Tukey's boundary can be accessed:

```{r}
hampel_outlier(heart_disease$age)
```

It returns a two-value vector. And there we have the bottom and the top threshold. All below 29.31 and all above 82.68 will be considered as an outlier.

The value 3 can be changed, but not in the `prep_outliers` function. This value is the most common.

  
<br>
  
## What to do with the outliers?

Regardless the choosed method, the `prep_outliers` function covers the two typical scenarios through parameter `type`:

* Case 1: Descriptive statistics / data profiling
* Case 2: Data for the predictive model


### Case 1: Data profiling: (`type='set_na'`)

In this case all outliers are converted into `NA`, thus applying most of the descriptive functions (max, min, mean, etc) will return a **less-biased indicator** value. Remember to set `na.rm=TRUE` parameter in each function otherwise the result will be `NA`.

For example, let's consider the following variable (the one we saw at the beginning):

```{r}
## To understand all of these metrics, please go to the Profiling data chapter: http://livebook.datascienceheroes.com/exploratory_data_analysis/profiling.html
profiling_num(df_1$var)
```

Here we can see several indicators that give us some clues. The `std_dev` is really high comparing with the `mean`, and it is reflected on the `variation_coef`. Also the `kurtosis` is high (18). And the `p_99` is almost twice than the `p_95` value (5718 vs 2972).

_This last analysis is like imaging a picture by what other person tell us, we convert the voice (which is a signal), into an image in our brain._ `r emo::ji("speaking_head")` `r emo::ji("roll_eyes")` ... `r emo::ji("mountain_snow")` 

<br>

**Tukey's method** 

```{r}
df_1$var_tukey=prep_outliers(df_1$var, type = "set_na", method = "tukey")
```

Now, we check how many `NA`s value are there.

```{r}
# before
df_status(df_1$var) %>% select(variable, q_na, p_na)

# after
df_status(df_1$var_tukey) %>% select(variable, q_na, p_na)
```

Before the transformation, there were 0 `NA`s values, and after 84 values (around 8%) were spoted as outliers according to the Tukey's test, and they were reeplaced by `NA`.

And we can compare the before and after:

```{r}
profiling_num(df_1) %>% select(variable, mean, std_dev, variation_coef, kurtosis, range_98)
```

The `mean` decreased by almost the half. And all the other metrics decreased as well.

**Hampel's method**:

Let's see what happen with Hampel's method:

```{r}
df_1$var_hampel=prep_outliers(df_1$var, type = "set_na", method="hampel")
```

```{r}
df_status(df_1) %>% select(variable, q_na, p_na)
```

This last method was much more severed spotting outliers. 36.7% of values were spotted as outliers. Probably because the variable is _quite_ skewed to the left.

More info can be found at Ref. [2]. 

<br>

**Top 2% method**

Finally we can try the easiest method, to remove the top 2%. 

```{r}
df_1$var_top1=prep_outliers(df_1$var, type = "set_na", method="bottom_top", top_percent = 0.02)
```

Please note that the 2% value was arbitrary choosed. Other values like 3% or 0.5% can be tried as well.

<br> 

#### Putting all together

```{r}
df_status(df_1) %>% select(variable, q_na, p_na)
prof_num=profiling_num(df_1) %>% select(variable, mean, std_dev, variation_coef, kurtosis, range_98)
prof_num
```



**Plotting**

```{r, outliers_method_comparison, warning=FALSE, message=FALSE}
# First we need to convert the dataset into wide format
df_1_m=reshape2::melt(df_1) 
plotar(df_1_m,  str_target= "variable", str_input = "value", plot_type = "boxplot")
```

<br>


When selecting the bottom-top x% we will always have some values matching that condition. But with the others it has not to be


#### Conclusions for dealing with outliers in data profiling 

The idea is to touch the outliers as less as it can be. So in this case, if we were interested only in describing the general behavior.

To accomplish this task, for example to create an ad-hoc report, we can use the `mean`. We could choose the top 2% method, since this it only affects to 2% of all values, and causes the `mean` to be lowered drastically. From 569 to 453, **26% less**.

Please note that this demostration doesn't mean that Hampel nor Tukey are a bad choice. In fact they are more robust since the threshold can be higher than current value. As a matter of fact, no value is treated as an outlier.

For example:

```{r}
# Getting outliers threshold
tukey_outlier(heart_disease$age)

# Getting  min and max values
min(heart_disease$age);max(heart_disease$age)
```

* The bottom threshold is 9, and the minimum value is 29. 
* The top threshold is 100, and the minimum value is 77.

Therfore: `age` variable has not outliers.

If we were used the bottom/top, it would have been detected the input percentages as outliers.

All the examples so for have been taking one variabel at a time, but `prep_outliers` can handle several at the same time using the parameter `str_input` as we will see in next section. All what we saw up to here will be equivalen, except for what we do once we detect the outlier, in other words, the imputation method. 

<br>
  
### Case 2: Outliers in predictive models (`type='stop'`)

The previous case causes that all spoted outliers are converted into `NA`s values. This is a huge problem if we are building a machine learning model since many of them don't work with `NA`s values.

To deal with outliers in order to use in a predictive model, we can adjust the paramter `type='stop'` so all values flagged as outliers will be converted to the threshold value.

**Some things to keep in mind:**
  
Try to think variables treatment (and creation) as if you're explaining to the model. Stopping variables at a certain value, 1% for example, we are telling to the model: _consider all extremes values as if they are on the 99% percentile, this value is already high enough._

Some model are more tolerant to noisy data than others, we can help them by treateating some of the outliers values. In practice it tends to be more resitant to new data, improving the inner pattern finding.

<br>

### Imputing outliers for predictive modeling

First we create a dataset with some outliers. Now the example has two variables.
```{r outliers_treatment1,  fig.height=3, fig.width=4}
# Creating data frame with outliers
options(scipen=999) # deactivating scientific notation
set.seed(10) # setting the seed to have a reproducible example
df_2=data.frame(var1=rchisq(1000,df = 1), var2=rnorm(1000)) # creating the variables
df_2=rbind(df_2, 135, rep(400, 30), 245, 300, 303, 200) # forcing outliers
```

Dealing with outliers in both variables, `var1` and `var2` using Tukey's method:

```{r outliers_treatment3,  fig.height=3, fig.width=4}
df_2_tukey=prep_outliers(data = df_2, str_input = c("var1", "var2"), type='stop', method = "tukey")
```

Checking the before and after:

```{r outliers_treatment3,  fig.height=3, fig.width=4}
profiling_num(df_2) %>% select(variable, mean, std_dev, variation_coef)
profiling_num(df_2_tukey) %>% select(variable, mean, std_dev, variation_coef)
```

Tukey worked perfect this time, exposing the _real_ mean in both variables: `1` for `var1` and `0` for `var2`.

Note this time there are no one `NA` values. What the function did was to assign the min and max values with the threshold 

Checking thershold for `var1`:
```{r}
tukey_outlier(df_2$var1)
```

Now checking the min max, before and after the transformation:
```{r}
# before:
min(df_2$var1);max(df_2$var1)

# after
min(df_2_tukey$var1);max(df_2_tukey$var1)
```

The min remains the same, at `0.0000031` but the maximum was set to the Tukey's value: `5.2`. 

The top 5 highest values before the prepartion were: 
```{r}
# before
tail(df_2$var1[order(df_2$var1)], 5)
```

But after...
```{r}
# after:
tail(df_2_tukey$var1[order(df_2_tukey$var1)], 5)
```

And checking there is no one `NA`:

```{r}
df_status(df_2_tukey) %>% select(variable, q_na, p_na)
```

Pretty clear, right?

<br>

Now let's replicate the example just like last section. Only with one variable:

```{r outliers_treatment3,  fig.height=3, fig.width=4}
df_2$tukey_var2=prep_outliers(data=df_2$var2, type='stop', method = "tukey")
df_2$hampel_var2=prep_outliers(data=df_2$var2, type='stop', method = "hampel")
df_2$bot_top_var2=prep_outliers(data=df_2$var2, type='stop', method = "bottom_top", bottom_percent=0.01, top_percent = 0.01)
```

<br>

#### Putting all together

```{r}
# excluding var2
df_2_b=select(df_2, -var1)

# profiling
profiling_num(df_2_b) %>% select(variable, mean, std_dev, variation_coef, kurtosis, range_98)
```

All the three methods show very similar results with this data.

**Plotting**


```{r, outliers_method_comparison, warning=FALSE, message=FALSE}
# First we need to convert the dataset into wide format
df_2_m=reshape2::melt(df_2_b) %>% filter(value<1000) 
plotar(df_2_m,  str_target= "variable", str_input = "value", plot_type = "boxplot")
```

_Important_: The two points obove the value 1000, only for `var1`, were excluded otherwise it was impossible to appreaciate the difference between the methods.

<br>


## References

* [1] http://datapigtechnologies.com/blog/index.php/highlighting-outliers-in-your-data-with-the-tukey-method/
* [2] http://exploringdatablog.blogspot.com.ar/2013/02/finding-outliers-in-numerical-data.html