High Cardinality Variable in Predictive Modeling
===

```{r ,results="hide", echo=FALSE}
library(knitr)
#opts_knit$set(base.dir = "data_preparation") ## TODO: DESCOMENTAR ANTES DE PUBLICAR
```

 
<style type="text/css">
.table {
    width: 40%;
}
</style>

### What is this about?

As we've seen in the other chapter (TODO: add link) we keep the categories with the major representative. But how about having another a variable to predict? That is, to predict `has_flu` based on `country`.

Using last method may destroy the information of the variable, thus lose predictive power. 

<br>

### But is it necesary to re-group the variable?

It depends on the case, but quick answer is yes. In this chapter we will see one case in which this data preparation increases overall accuracy (measure by Area Under Roc Curve).

There is a tradeoff between the **representation of the data** (how many rows has each category), and how each category is related to an the outcome variable. E.g.: some countries are more related to persons with flu than others.

```{r , message=F}
# Loading funModeling which contains functions to deal with this.
library(funModeling) 
```


```{r}

# plotting first 10 rows
head(data_country)

# exploring data, excluding person id.
Hmisc::describe(data_country[,-1])
```

<br>

### The case :mag_right:

The predictive model will try map certain values with certain outcomes, in our case target variable is binary, roughly speaking: 


```{r}
## `categ_analysis` is available in "funModeling" >= v1.6, please install it before using it.
country_profiling=categ_analysis(data=data_country, str_input="country", str_target = "has_flu")

## Displaying first 6 elements
#head(country_profiling)
country_profiling
```

We've a complete profiling of `country` regarding the target variable `has_flu`. Each row represent an unique category of `str_input` variable, and each row an attribute that defines each category in terms of representativity and likelihood.  

Note 1: _First 4 columns are automatically adjust to the names specified in `str_input` and `str_target`, we're going to use the names for this examples, but keep in mind if we specified other parameters this names will change. Last 2 columns will remain always the same._

Note 2: _`has_flu` variable has values `yes` and `no`, `categ_analysis` assigns internally the number 1 to the less representative class, `yes` in this case, in order to calculate the mean, sum and percentage._

These are the metrics returned by `categ_analysis`:

* `country`: name of each category in `str_input` variable
* `sum_has_flu`: summatory of `has_flu="yes"` values
* `perc_has_flu`: share of `yes` values across categories. This column sums `1.00`.
* `mean_has_flu`: average number of `has_flu="yes"` for that category. It is the likelihood.
* `q_rows`: quantity of rows that, regardless the `has_flu` variable, fell in that category. It's the distribution of `str_input`. This column sums the total of rows analyzed.
* `perc_rows`: related to `q_rows` it represents the share of each category. This column sums `1.00`

**What conclussions can we draw from this?**

Reading example based on 1st row `France`: 

* 41 persons has flu (`sum_has_flu=41`). These 41 persons represent almost 50% of total persons having flu (`perc_has_flu=0.494`). 
* Likelihood of having flu in France is 14.2% (`mean_has_flu=0.142`)
* Total rows from France=288 -out of 910-. This is the `q_rows` variable; `perc_rows` is the same number but in percentage.

Regardless the filter by country, we've got:

* Column `sum_has_flu` sums the total persons with flu present in data.
* Column `perc_has_flu` sums `1.00` -or 100%
* Column `q_rows` sums total rows present in `data_country` data frame.
* Column `perc_rows` sums `1.00` -or 100%.


<br>

#### Don't predictive models handle this?

We're going trough this by building two predictive models: Gradient Boosting Machine -quite robust across many different data inputs. 

First model has not treated data, and second one has been treated by the function in `funModeling` package.

We're measuring the precision based on ROC area, ranged from 0.5 to 1, the higher the number the better the model is. We are going to use cross-validation to be more _sure_ about the value. [1]

```{r}
## Building the first model, without reducing cardinality.
library(caret)
fitControl <- trainControl(method = "cv", 
                           number = 4, 
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary)


fit_gbm_1 <- train(has_flu ~ country, 
                 data = data_country, 
                 method = "gbm", 
                 trControl = fitControl,
                 verbose = FALSE,
                 metric = "ROC")

sprintf("Area under ROC curve is: %s", round(max(fit_gbm_1$results$ROC),2))
```


Now we do the same model with the same parameters, but with the data preparation we did before.

<br> 


```{r}
## Building the second model, country_group is based on country.
fit_gbm_2 <- train(has_flu ~ country_group, 
                   data = data_country, 
                   method = "gbm", 
                   trControl = fitControl,
                   verbose = FALSE,
                   metric = "ROC")


sprintf("Area under ROC curve is: %s", round(max(fit_gbm_2$results$ROC),2))

```

Now the ROC is 0.74, which is xxx% higher than previous model.

We've used one of the most robust models, **gradient boosting machine**, and we've increased the performance. If we try other model, for example logistic regression, which is more sensible to dirty data, we'll get a higher difference between reducing and not reducing cardinality. This can be checked deleting `verbose=FALSE` parameter and changing `method=glm` (`glm` implies logistic regression).

<br>

### What we've got "for-free" from the grouping?

Knowing how categories fell into groups give us information that -in some cases- is good to report. Each category between the group will share similar behavior -in terms of representativity and prediction power-. 

If `Argentine` and `Chile` are in `group_1`, then they are the same, and this is how the model will _see_ it.

<br>

### Final toughts

* _Should we always reduce the cardinality?_ It depends, Two tests on a simple data are not enough to extrapolate to all cases. Hopefully it will be a good kick-off to the reader to start doing her/his own optimizations.

* What was mention at the begining respect to **destroy the information in the input variable**, implies that the resultant grouping have the same rates across groups (in a binary variable input). [2]

* The trade-off between representation of the data, and predictive power can be seen as a _fractal_ to what is desiered when building a predictive model, a correct balance between complexity (quantity of variables), and predictive power without incurring in overfitting. Re-grouping categorical variables helps to **reduce overfiting**.

<br>

**References:**

* [1] The importance of cross-validation is explained in the: <a href="http://livebook.datascienceheroes.com/model_performance/knowing_the_error.html">Knowing the Error</a> chapter.
* This can be study using the function described in the chapter: <a href="http://livebook.datascienceheroes.com/selecting_best_variables/cross_plot.html">Cross Plot (data viz)</a>.

