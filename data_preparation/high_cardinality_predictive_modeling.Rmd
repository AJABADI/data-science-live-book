High Cardinality Variable in Predictive Modeling
===

```{r ,results="hide", echo=FALSE}
library(knitr)
#opts_knit$set(base.dir = "data_preparation") ## TODO: DESCOMENTAR ANTES DE PUBLICAR; library("funModeling",lib.loc="/Users/oblaphouses/Library/R/3.3/library")
```

 
<style type="text/css">
.table {
    width: 40%;
}
</style>

### What is this about?

As we've seen in the other chapter (TODO: add link) we keep the categories with the major representative. But how about having another a variable to predict? That is, to predict `has_flu` based on `country`.

Using last method may destroy the information of the variable, thus lose predictive power. 

<br>

### But is it necesary to re-group the variable?

It depends on the case, but quick answer is yes. In this chapter we will see one case in which this data preparation increases overall accuracy (measure by Area Under Roc Curve).

There is a tradeoff between the **representation of the data** (how many rows has each category), and how each category is related to an the outcome variable. E.g.: some countries are more related to persons with flu than others.

```{r , message=F}
# Loading funModeling which contains functions to deal with this.
library(funModeling) 
library(dplyr)
```


```{r}

# plotting first 10 rows
head(data_country)

# exploring data
Hmisc::describe(data_country)
```

<br>

### The case :mag_right:

The predictive model will try map certain values with certain outcomes, in our case target variable is binary, roughly speaking: 


```{r}
## `categ_analysis` is available in "funModeling" >= v1.6, please install it before using it.
country_profiling=categ_analysis(data=data_country, input="country", target = "has_flu")

## Displaying first 6 elements
#head(country_profiling)
country_profiling
```

We've a complete profiling of `country` regarding the target variable `has_flu`. Each row represent an unique category of `input` variable, and each row an attribute that defines each category in terms of representativity and likelihood.  

Note 1: _First 4 columns automatically adjust their names based on `input` and `target`, we're going to use the names for these examples, but keep in mind if we specified other parameters this names will change. Last 2 columns will remain always the same._

Note 2: _`has_flu` variable has values `yes` and `no`, `categ_analysis` assigns internally the number **1** to the less representative class, `yes` in this case, in order to calculate the mean, sum and percentage._

These are the metrics returned by `categ_analysis`:

* `country`: name of each category in `input` variable
* `sum_target`: summatory of `has_flu="yes"` values
* `perc_target`: share of `yes` values across categories. This column sums `1.00`.
* `mean_target`: average number of `has_flu="yes"` for that category. It is the likelihood.
* `q_rows`: quantity of rows that, regardless `has_flu` variable, fell in that category. It's the distribution of `input`. This column sums the total rows analyzed.
* `perc_rows`: related to `q_rows` it represents the share or percentage of each category. This column sums `1.00`

<br>

#### What conclussions can we draw from this?

Reading example based on 1st row, `France`: 

* 41 people have flu (`sum_target=41`). These 41 people represent almost 50% of total people having flu (`perc_target=0.494`). 
* Likelihood of having flu in France is 14.2% (`mean_target=0.142`)
* Total rows from France=288 -out of 910-. This is the `q_rows` variable; `perc_rows` is the same number but in percentage.

Regardless the filter by country, we've got:

* Column `sum_target` sums the total people with flu present in data.
* Column `perc_target` sums `1.00` -or 100%
* Column `q_rows` sums total rows present in `data_country` data frame.
* Column `perc_rows` sums `1.00` -or 100%.

<br>

### Analysis for Predictive Modeling 

When developing predictive models, we may be interested in those values which increases the likelihood of certain event. In our case: 

**What are the countries which maximizes the likelihood of finding people with flu?**

Easy, take `country_profiling` in a descending order by `mean_target`:

```{r}
# Ordering country_profiling by mean_target and then take the first 6 countries
arrange(country_profiling, -mean_target) %>%  head(.)
```

<br>

Great! We've got `Malasyia` as the country with the highest likelihood to have flu! 100% of people there having flu (`mean_has_flu=1.000`).

But our common sense advices us that _perhaps_ something is wrong... 

How many rows does Malasya have? Answer: 1. -column: `q_rows=1`
How many positive cases does Malasya have? Answer: 1 -column: `sum_target=1`

Since the sample cannot be increased thus see if this proportion keeps high, it will contribute to **overfit** and bias the predictive model. 

How about `Mexico`? 2 out of 3 have flu... it seems still low. However `Uruguay` has 17.3% of likelihood -11 out of 63 cases- and these 63 cases represents almost 7% of total population (`perc_row=0.069`), well this ratio seems more credible.

Next there are some ideas to treat this:

#### Case 1: Reducing by re-categorizing less representative values

Keep all cases with at least certain % of representation in data. Let's say to rename those countries which have less than 1% of presence in data to `others`.

Example of this can be found at: TODO: INSERT LINK TO ...

```{r}
country_profiling

country_profiling=categ_analysis(data=data_country, input="country", target = "has_flu")

countries_high_rep=filter(country_profiling, perc_rows>0.01) %>% .$country

## If not in countries_high_rep then assign `other` category
data_country$country_new=ifelse(data_country$country %in% countries_high_rep, data_country$country, "other")

```

Checking again the likelihood: 

```{r}
country_profiling_new=categ_analysis(data=data_country, input="country_new", target = "has_flu")
country_profiling_new
```

We've reduced the quantity of countries drastically -**74% less**- only by shrinking less representative at 1%. Obtaining 18 out of 70 countries.

Likelihood of target variable has been stabilized a little more in `other` category. Now when the predictive model _sees_ `Malasya`  it will **not assign 100% of likelihood, but 4.1%** (`mean_has_flu=0.041`).

**Advice about last method:**

Watch out about applying this technique blindly. Sometimes in **high unbalanced** target prediction -e.g. **anomaly detection**- the abnormal behavior is present in less than 1% of cases.

```{r}
d_abnormal=data_country
d_abnormal$abnormal=ifelse(d_abnormal$country %in% c("Brazil", "Bulgaria", "Chile"), 1, 0) 
categ_analysis(d_abnormal, input = "country", target = "abnormal") %>% arrange(-mean_target)
```

If  
TODO: COMPLETE

<br>

#### Case 2: Reducing by automatic grouping

This procedure uses the `kmeans` clustering technique and the table returned by `categ_analysis` in order to create groups -clusters- which contain categories which exhibit similar behavior in terms of:

* `sum_target`
* `mean_target`
* `perc_rows`

The combination of all of them will lead to find groups considering likelihood and representativity.


**Hands on R:**

We define `n_groups` parameter, it's the number of desiered groups. The number is relative to the data and the number of total categories. But a general number would be between 3 and 10.

Function `auto_grouping` comes in `funModeling` >=1.6. Please note that `target` parameter only supports by now binary variable.

_Note: `seed` parameter is optional, assigning a number will retrieve always the same results._
```{r}
## Reducing the cardinality
country_groups=auto_grouping(data = data_country, input = "country", target="has_flu", n_groups=8, seed = 999)
```

`auto_grouping` returns a list containing 3 objects:

`recateg_results`: Is a data frame with is useful to profile each group (`country_rec`). The predictive model will _see_ these groups:

```{r}
country_groups$recateg_results
```

Last table is ordered by mean_target, so we can quickly see groups maximizing and minimizing the likelihood. 

We'll analyze `group_5` at the end.

* `group_3` is the most common, it is present in 31.6% of cases and mean_target (likelihood) is 14.2%. 
* Eexcluding `group_5` by now, `group_4` has the highest likelihood, while `group_8` has the lowest. Both have good representativity: 11.9 and 14.7 of all input rows.
* `group_6` and `group_8` are pretty similar, they can be one group since likelihood is 0 in both cases.

**How about `group_5`?**

We see that is the group with the most likelihood, 75% `has_flu`. This is a cluster of outlier, here are the categories with low-representativity and high likelihood. `Malasia` and `Mexico` are here. 

If we are more cautelous about false positive, we can consider that this group has not enough information and assign it to `group_8` so it will have no influence to increase likelihood of predicting flu, because `mean_target=0`. Or, we can assign to an average group like `group_3`.

```{r}
data_country=data_country %>% inner_join(country_groups$df_equivalence)
```

 
### How about handling new categories when the predictive model is on production?

Let's imagine a new country appears, `new_country_hello_world`, predictive models will fail since they were trained with fixed values. One technique is to assign a group which has `mean_target=0`. 

It's similar to the case in last example. But the difference lies in `group_5` would fit better in a mid-likelihood group than a complete new value.

After some time we should re-build the model with all new values, otherwise we would be penalizing `new_country_hello_world` if it has a good likelihood.

<br>

### Don't predictive models handle high cardinality?

We're going trough this by building two predictive models: Gradient Boosting Machine -quite robust across many different data inputs. 

First model has not treated data, and second one has been treated by the function in `funModeling` package.

We're measuring the precision based on ROC area, ranged from 0.5 to 1, the higher the number the better the model is. We are going to use cross-validation to be more _sure_ about the value. [1]

```{r}
## Building the first model, without reducing cardinality.
library(caret)
fitControl <- trainControl(method = "cv", 
                           number = 4, 
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary)


fit_gbm_1 <- train(has_flu ~ country, 
                 data = data_country, 
                 method = "gbm", 
                 trControl = fitControl,
                 verbose = FALSE,
                 metric = "ROC")

sprintf("Area under ROC curve is: %s", round(max(fit_gbm_1$results$ROC),2))
```


Now we do the same model with the same parameters, but with the data preparation we did before.

<br> 


```{r}
## Building the second model, country_group is based on country.
fit_gbm_2 <- train(has_flu ~ country_rec, 
                   data = data_country, 
                   method = "gbm", 
                   trControl = fitControl,
                   verbose = FALSE,
                   metric = "ROC")


sprintf("Area under ROC curve is: %s", round(max(fit_gbm_2$results$ROC),2))

```

Now the ROC is 0.74, which is xxx% higher than previous model.

We've used one of the most robust models, **gradient boosting machine**, and we've increased the performance. If we try other model, for example logistic regression, which is more sensible to dirty data, we'll get a higher difference between reducing and not reducing cardinality. This can be checked deleting `verbose=FALSE` parameter and changing `method=glm` (`glm` implies logistic regression).

<br>

### What we've got "for-free" from the grouping?

Knowing how categories fell into groups give us information that -in some cases- is good to report. Each category between the group will share similar behavior -in terms of representativity and prediction power-. 

If `Argentine` and `Chile` are in `group_1`, then they are the same, and this is how the model will _see_ it.

<br>

### Final toughts

* _Should we always reduce the cardinality?_ It depends, Two tests on a simple data are not enough to extrapolate to all cases. Hopefully it will be a good kick-off to the reader to start doing her/his own optimizations.

* What was mention at the begining respect to **destroy the information in the input variable**, implies that the resultant grouping have the same rates across groups (in a binary variable input). [2]

* The trade-off between representation of the data, and predictive power can be seen as a _fractal_ to what is desiered when building a predictive model, a correct balance between complexity (quantity of variables), and predictive power without incurring in overfitting. Re-grouping categorical variables helps to **reduce overfiting**.

<br>

**References:**

* [1] The importance of cross-validation is explained in the: <a href="http://livebook.datascienceheroes.com/model_performance/knowing_the_error.html">Knowing the Error</a> chapter.
* This can be study using the function described in the chapter: <a href="http://livebook.datascienceheroes.com/selecting_best_variables/cross_plot.html">Cross Plot (data viz)</a>.

