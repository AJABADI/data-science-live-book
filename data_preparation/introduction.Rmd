Introduction
===

### What is this about?

In practice, 90% of the time is spent in data preparation this book</a> does't have -yet- empirical information about how model performance increases by preparing the data.

```{r ,results="hide", echo=FALSE}
library(knitr)
opts_knit$set(base.dir = "data_preparation")
#opts_chunk$set(cache=TRUE, cache.path = 'DocumentName_cache/', fig.path='figure/')
```


### Removing outliers 

**Model building**: Some models such as random forest and gradient boosting machines tend to deal better with outliers, but some noise will affect results anyway. 

**Communicating results:** If we need to report the variables used in the model, we'll end up removing outliers to not see an histogram with only one bar, and/or show not a biased mean. 

It's better to show a non-biased number than justifying the model _will handle_ extreme values.

**Type of outliers:** 

* Numerical: For example the ones which bias the mean.
* Categorical: Having a variable in which the dispersion of categories is quite high (high cardinallity). For example: postal code.

Jump to <a href="http://livebook.datascienceheroes.com/data_preparation/outliers_treatment.html">Outliers Treatment</a> chapter.

<br>

### Don't use information from the future

<img src="back_to_the_future.png" width='250px'> 

Common mistake when starting a new predictive model project, for example:

Imagine we need to build a predictive model to know what users are likely to adquire full subscription in a web application, and this software has a ficticious feature called it `Feature A`:


```{r echo=FALSE}
d1=data.frame(user_id=rep(1:10), 
              feature_A=c("yes","yes","yes","no","yes","no","no","no","no","no"),
              full_subscription=c("yes","yes","yes","no","yes","no","no","no","no","no")) 
knitr::kable(d1, digits = 2)
```


We build the predictive model, we got a perfect accuracy, and an inspection throws the following: _"100% of users that have full subscription, uses Feature A"_. Some predictive algorithms report variable importance, thus `feature_A` will be at the top.

**The problem is:** `feature_A` is only availble **after the user goes for full subcription**. Therefore it cannot be used.

**The key message is**: Don't trust in perfect variables, nor perfect models. 

<br>

### Play fair with data, let it to develop their behavior

If a **numerical variable** increases as time moves, we may need to define an **observation time window** to analyze or when creating the predictive model, thus not biasing results. 

* Setting the **minimun** time: How much is it need to start seeing the behavior? 
* Setting the **maximun** time: How much time is it needed to see the end of the behavior? 

Eassiest solutions are: setting minimun since begin, and the maximun as the whole history.

<font color="#5d6d7e">

**Example:** 

```{r echo=FALSE, fig.height=2, fig.width=3}
library(ggplot2)
d4=data.frame(user=c('Laura','Laura','Laura','Laura','Laura','Tim','Tim','Tim','Tim','Tim', 'Laura','Laura'), days_since_signup=c(1,2,3,4,5,1,2,3,4,5,6,7), feature_A=c(2,3,3,5,12, 0, 0, 1, 6, 15, 20,24), stringsAsFactors = F)
pd4=ggplot(d4, aes(x=days_since_signup, y=feature_A, colour=user)) + geom_line() +  scale_x_continuous(breaks = 1:7)
plot(pd4)
```


User `Laura` starts knowing `feature_A` from day 3, and after 5 days she has more use on this feature than `Tim` who started using it from day 0.  

If `Laura` adcquiere `full subscription` and `Tim` doesn't, _what would the model learn?_ 

If modeling with full history -`days_since_signup = all`-, the higher the `days_since_signup` the higher the likelihood, since `Laura` has the highest number.

However, if we keep only with the user history corresponding to their first 5 days, the conclussion is the opposite. 



</font>


------
